<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://dvdjsp.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://dvdjsp.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-21T19:25:42+00:00</updated><id>https://dvdjsp.github.io/feed.xml</id><title type="html">Davidson Noby Joseph</title><subtitle>A Curious Grad Student in Physics </subtitle><entry><title type="html">What the heck is \(\chi^2\) anyway?</title><link href="https://dvdjsp.github.io/blog/2026/Chisq/" rel="alternate" type="text/html" title="What the heck is \(\chi^2\) anyway?"/><published>2026-01-16T00:00:00+00:00</published><updated>2026-01-16T00:00:00+00:00</updated><id>https://dvdjsp.github.io/blog/2026/Chisq</id><content type="html" xml:base="https://dvdjsp.github.io/blog/2026/Chisq/"><![CDATA[<p><em>How do you fit a curve through data? What‚Äôs a statistic? and most importantly <strong>what is being squared?</strong></em></p> <p>These are important questions that you might encounter when you look up curve fitting! This is a continuation on <em>how to fit data</em> where I answer the former in <a href="/blog/2026/LeastSq/">Least-squares and fitting curves through points</a>. Before getting into what a statistic is, let‚Äôs first begin by answering the most common and important question here:</p> <style>.question-box{border:2px solid #a463bf;border-radius:8px;padding:20px;margin-bottom:25px;background-color:#f9f9f9}.question-box h3{color:#a463bf;margin-top:0;font-size:1.3em;font-weight:700}.question-content{font-size:1.05em;line-height:1.6;color:#333;margin-bottom:15px}details{border:1px solid #ddd;border-radius:6px;margin-top:15px;background-color:#fff}details>summary{cursor:pointer;padding:12px 15px;background-color:#a463bf;color:white;font-weight:600;font-size:1.05em;border-radius:5px;transition:background-color .2s}details>summary:hover{background-color:#8b4ba3}details[open]>summary{border-radius:5px 5px 0 0;margin-bottom:15px}.solution-content{padding:15px;line-height:1.7;color:#333}.solution-content p{margin-bottom:10px}.page-header{text-align:center;margin-bottom:40px;padding-bottom:20px;border-bottom:3px solid #a463bf}.page-header h1{color:#a463bf;margin-bottom:10px}.page-header p{color:#666;font-size:1.1em}html[data-theme='dark'] .question-box{background-color:#1a1a1a;border-color:#2698ba}html[data-theme='dark'] .question-box h3{color:#2698ba}html[data-theme='dark'] .question-content{color:#e0e0e0}html[data-theme='dark'] details{background-color:#2a2a2a;border-color:#444}html[data-theme='dark'] details>summary{background-color:#2698ba}html[data-theme='dark'] details>summary:hover{background-color:#3bb5d9}html[data-theme='dark'] .solution-content{color:#e0e0e0}html[data-theme='dark'] .page-header h1{color:#2698ba}html[data-theme='dark'] .page-header p{color:#b0b0b0}</style> <div class="question-box"> <h3 id="what-is-being-squared">What is being squared?</h3> <p><strong>Nothing!</strong> This is a trick question used by cruel people. Unfortunately, is a great example of an abuse of notation: \(\chi^2\) is a variable, just like \(x\) or \(y\). We could have called it \(\chi\), but for historical reasons, <a href="https://www.tandfonline.com/doi/abs/10.1080/14786440009463897">Pearson called it \(\chi^2\).</a></p> </div> <p>Now that we‚Äôve tackled this important question, let us motivate the statistic with the same example we used previously:</p> <p>You (and your friend) are trying to measure the resistance of a resistor by studying its Voltage vs Current using the relation \(V=I R\). Unfortunately, your equipment isn‚Äôt the greatest and you get noisy data that is ‚Äúroughly‚Äù linear. Adding to that, neither of your sets of measurements are identical, i.e not only are they not on a line, they are different for the both of you!</p> <pre><code class="language-plotly">{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 15,
      "mode": "markers",
      "name": "Trial 1",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "First Measurement"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
</code></pre> <pre><code class="language-plotly">{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 15,
      "mode": "markers",
      "name": "Trial 2",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Second Measurement"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
</code></pre> <p>It seems that there isn‚Äôt <em>just</em> an uncertainty that prevents the data from landing perfectly on a line, but also in addition, each data point <strong>has uncertainties</strong></p> <p>Earlier we laid out the ideas behind curve fitting, but now we can now answer the more physical question of fitting data <strong>with uncertainties</strong> which is where \(\chi^2\) comes into play. The \(\chi^2\) statistic (pronounced <strong>chi-squared</strong>) is a metric used to asses a fit to a collection of data. To work with \(\chi^2\), we first need some <strong>important</strong> ideas from statistics so we‚Äôll start here. If you want to skip to using it, then click on <a href="#how-chi2-goodness-of-fit-is-used">how to use \(\chi^2\) goodness of fit</a>.</p> <h1 id="some-statistics-overview"><span style="color: #a463bf;">Some statistics overview</span></h1> <div class="question-box"> <h3 id="dr-strange-and-the-very-many-different-outcomes">Dr. Strange and the very many different outcomes</h3> <p>You put on your thinking cap and recognize that each of the data point you measure is really one possible outcome that you experience. Like Dr. Strange using his ability to view all the possible multiverses, you view each data point you see as a specific outcome, and the collection of all possible outcomes together is what you would get if you repeat the experiment infinitely many times. In this sense, we say that the the data points are <strong>sampled from a distribution</strong> which controls the probability of it showing up as a special number. Mathematically, what this means is that all the data points \(y_i\) are <strong>random variables</strong>.</p> <p>What this implies is that the more data you measure, the more information you get about each data point and therefore the more accurate your results become. The true values of each data point should be the mean of all the data you have for that particular data point. There The variation is its standard error of the mean and you plot them together with error bars.</p> </div> <p>To get an intuitive feel for what this means, lets think about it in terms of an excel sheet. If you imagine an excel sheet keeping track of your measured data with each row corresponding to its trial number and each column being the measured values for the \(i^{\rm th }\) data point \(y_i\), then isolating a column (say the first column of \(y_1\)) over <strong>all trials</strong> gives you the different possible values that data point can \(y_i\) take. How do we use this information? We <strong>average!</strong></p> <p>The idea is to collapse the information of multiple plots by aggregating a specific column and plotting the corresponding averages as data points \(\bar{y}_i\)</p> <pre><code class="language-plotly">{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 15,
      "mode": "markers",
      "name": "Data ",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      },
      "error_y": {
        "type": "constant",
        "value": 1.0,
        "visible": true,
        "color": "#a463bf",
        "thickness": 2,
        "width": 4
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Voltage vs Current with Error Bars"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
</code></pre> <p>where we <strong>call</strong> the error bars of size \(\sigma_{\bar{y}_i}\) as the standard error of the mean for each column.</p> <details style="padding: 15px 20px;"> <summary>Population and Sample mean, error and standard error </summary> <p>In statistics, a lot of similar sounding terms can be thrown around that can lead to confusion. Here I would like to clarify the difference between <strong>population</strong> and <strong>sample</strong> data set. The quantities ‚Äúpopulaton mean‚Äù, ‚Äúsample standard deviation‚Äù etc are known as a measured <strong>statistic</strong>.</p> <p>In a nutshell, the population data set is the true data set, everyone and everything that you seek to study is present in the data set. Its mean (called the <strong>population mean</strong> denoted by \(\mu\)) and standard deviation (called the <strong>population standard deviation</strong> denoted by \(\sigma\)) is the <strong>true</strong> or correct statistic. However, in real life, it‚Äôs often not possible to get the whole collection, so we work with a small subset called the <strong>sample data set</strong>, i.e <strong>your</strong> data set.</p> <ul> <li> <p>The sample mean \(\bar{s}\) corresponds to the mean of your column, it is what we call an <strong>estimator</strong> of the true population mean</p> </li> <li> <p>The sample standard deviation \(s\) is the standard deviation of your column computed using <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction">Bessel‚Äôs correction</a>, it is what we call an <strong>estimator</strong> of the true population standard deviation.</p> </li> </ul> <p>So far, all of this is purely based on the data you collect, and nothing to do with the population mean or variance directly, only that they approximate or estimate the true values. Now comes the sample standard error (or standard error for short) that acts as a bridge between your data and the whole data set,</p> <ul> <li> <p>The sample standard error is sort of a bridge between the population and sample data set: it tells you roughly how far your sample mean is compared to the actual, true population mean and is defined by</p> \[\sigma_{\bar{s}}= \frac{s}{\sqrt{N}}\] </li> </ul> </details> <p>Now that we know of a method to encode the information that we get form multiple trials through aggregating, we notice that for different data sizes, we get different means and so the means themselves are a random variable. So we ask:</p> <div class="question-box"> <h3 id="what-distribution-does-the-data-point-baryi-come-from">What distribution does the data point \(\bar{y}_i\) come from?</h3> <p>This question is a genuinely hard question that is not possible to answer exactly. The simplest answer is to make an educated guess. However, given certain conditions something truly spectacular happens: you can assume that the data point \(y_i\) comes from a <strong>Normal distribution</strong> \(\mathcal{N}(\mu_i,\sigma_i)\) through something called the <strong>Central Limit Theorem</strong>.</p> <p>Before we get into this, let‚Äôs think about the weight of this conclusion. Out of the many distributions you know like the humble <a href="">uniform distribution</a>, or the <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson</a> or the <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Lorentzian</a> or the many many more distributions you don‚Äôt know (I certainly didn‚Äôt until now) like the <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a> or the <a href="https://en.wikipedia.org/wiki/Continuous_Bernoulli_distribution">Continuous Bernoulli Distribution</a>, we arrive at the seemingly innocent Normal distribution!</p> <p>This Normal distribution or the ‚Äúbell shaped curve‚Äù with mean \(\mu\) and <strong>variance</strong> \(\sigma^2\) is given by the following equation</p> \[\mathcal{N}(\mu,\sigma^2)= \frac{1}{\sqrt{2\sigma^2 \pi}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}\] <p>and is the renowned ‚ÄúGaussian distribution‚Äù but normalized (i.e the area under this curve is \(=1\)).</p> <p>The Central Limit Theorem implies that the sample mean we were measuring \(\bar{y}_i\) using \(N\) trials comes from a normal distribution represented mathematically as</p> \[\bar{y}_i \sim \mathcal{N}\left(\mu,\frac{\sigma^2}{N}\right),\] <p>where \(\mu_i\) is the true population mean and \(\sigma_i\), the population standard deviation, both of which are unknown. However, just because they are unknown does not mean that we can‚Äôt approximate them. In fact the sample standard error of the mean (from your data set) is an estimator and can be used instead</p> \[\frac{\sigma^2}{N}=\sigma_{\mu}\approx \sigma_{\bar{s}}\] <details style="padding: 15px 20px;"> <summary>Central Limit Theorem: Why <u>the</u> Normal distribution?</summary> <p>The <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem (CLT)</a> is what we used to justify the emergence of the Normal distribution. This theorem is colloquially stated along the lines of <strong>‚Äúmore data means more bell shaped‚Äù</strong> but this isn‚Äôt quite the correct statement of the theorem and can lead to a lot of misunderstandings. There are a few conditions for the CLT to hold true, and they are nicely discussed in <a href="https://www.youtube.com/watch?v=zeJD6dqJ5lo">3B1B‚Äôs ‚ÄúBut what is the Central Limit Theorem?‚Äù</a> that I will paraphrase below:</p> <ol> <li> <p><strong>Independence</strong>: The data you collect should be independent of each other, i,e measuring one does not affect the next measurement</p> </li> <li> <p><strong>Same distribution</strong>: The data you collect, i.e the data points in the columns in your excel sheet should come from the same distribution</p> </li> <li> <p><strong>Finite variance</strong>: Your error bars must be finite, i.e more data does not increase the variance you observe for a single column.</p> </li> </ol> <p>Denote by \(y_i^{(j)}\) the data points in the \(i^{\rm th}\) column and \(j^{\rm th}\) row. Given the above conditions, CLT states that in the limit as you perform many many many trials (i.e the number of rows \(N\) goes to infinity),</p> <p>The quantity</p> \[\lim_{N\to \infty} \frac{\sum_{j=1}^N y_i^{(j)}}{N}\] <p>which is just the average of our column follows</p> \[\lim_{N\to \infty} \frac{\frac{\sum_{j=1}^N y_i^{(j)}}{N}- \mu_i}{\frac{\sigma_i}{\sqrt{N}}}\sim \mathcal{N}(0,1)\] <p>follows a Normal distribution with mean zero and standard deviation one!</p> <p>This means the average in a fixed column in the limit of more data points follows a normal distribution around the population mean \(\mu_i\) and standard deviation which is the standard error of the mean:</p> \[\lim_{N\to \infty} \frac{\sum_{j=1}^N y_i^{(j)}}{N}\sim \mathcal{N}\left(\mu_i,\frac{\sigma_i}{\sqrt{N}}\right)\] </details> </div> <p>All that jazz to say that for our fitting purposes, we know the kind of errors we <strong>need</strong> to account for in our real experiments.</p> <h1 id="how-chi2-goodness-of-fit-is-used"><span style="color: #a463bf;" id="how-chi2-goodness-of-fit-is-used">How \(\chi^2\) goodness of fit is used</span></h1> <p>But <em>how</em> do we account for this errors? If we <strong>didn‚Äôt</strong> have errors per data point, then we‚Äôd stick to our usual least-squares method and minimize the least squares. Essentially, we‚Äôre trying to make the distance between our model \(f(\boldsymbol{x},\boldsymbol{\theta})\) and our data points \(y_i\) <strong>small</strong>, but with errors, <em>what does small mean?</em> i.e, <strong>small with respect to what?</strong></p> <p>In the case where data points have errors, a natural ‚Äúscale‚Äù we‚Äôre trying to compare this is against the fluctuations‚Äìthe error \(\sigma_i\) per data point. The error is estimated using multiple trials for a data point or guesstimated based on the order of magnitude. With this scale in mind, we minimize this sort of modified or scaled least-squares which is what we call \(\chi^2\)</p> \[\begin{equation}\sum_{i=1}^N \left(\frac{y_i- f(\boldsymbol{x};\boldsymbol{\theta})}{\sigma_i}\right)^2:= \chi^2.\end{equation}\] <p>If we‚Äôre fitting a <strong>line</strong> to our data points with errors (as in the example of \(V\) vs \(I\)), then we‚Äôd use \(f(\boldsymbol{x};\boldsymbol{\theta})=f(x;m,c):=m x +c\) for the free parameters of the line \(m\) (slope) and \(c\) (intercept). Since errors \(\sigma_i\) are just (estimated or guesstimated) and comes in as a multiplicative factor to residues, the method to find the optimal solution is identical to what we did in <a href="/blog/2026/LeastSq/">Least-squares and fitting curves through points</a>. Since \(\chi^2\) is just least squares accounting for the error for each data point, in <code class="language-plaintext highlighter-rouge">python</code>, we‚Äôd still use <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html"><code class="language-plaintext highlighter-rouge">scipy.optimize.curve_fit()</code></a> with <code class="language-plaintext highlighter-rouge">sigma</code> (make sure to set <code class="language-plaintext highlighter-rouge">absolute_sigma=True</code>).</p> <p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">scipy.optimize.curve_fit()</code> does not return \(\chi^2\), it returns the parameters of your model i.e your fit. You‚Äôd have to compute \(\chi^2\) by using those parameters in Eq. (1)</p> <div class="question-box"> <h3 id="what-value-of-chi2-corresponds-to-a-good-fit">What value of \(\chi^2\) corresponds to a ‚Äúgood‚Äù fit?</h3> <p>\(\chi^2\) by itself for a particular fit does not tell us much: <em>Should it be very small? Should it be very big?</em> A common (but understandable) misconception is that since \(\chi^2\) comes from least-squares, obtaining the smallest value of \(\chi^2\) should correspond to a good fit since that worked for least squares. However, this is <strong>not</strong> true! The quantity you need to check against is the so called <strong>reduced chi-squared</strong> defined by</p> \[\chi^2_{\nu} := \frac{\chi^2}{\nu}\] <p>where \(\nu=N-p\) is called the <strong>degree of freedom</strong> that you can find by subtracting the number of data points with the number of parameters you used for the model. For example: If you had 10 data points that you‚Äôre trying to fit a line to, then \(\nu=10-2=8\).</p> <p><strong>What value of \(\chi^2_{\nu}\) is good?</strong> It turns out that the expected value (average) of \(\chi^2_{\nu}\) for a good fit is one, i.e</p> \[\mathbb{E}[\chi^2_{\nu}]= 1\] <p>meaning that a fit is ‚Äúgood‚Äù if</p> \[\chi^2_{\nu} \approx 1\] <details style="padding: 15px 20px;"> <summary>Why is \(\chi^2_{\nu}\) expected to be one?</summary> </details> </div> <p>Now that we‚Äôve equipped you with the basics of curve fitting, I wish you the best on your fitting adventures!</p>]]></content><author><name></name></author><category term="cool-math-stuff"/><category term="DataFitting"/><category term="Optimization"/><category term="Probability"/><summary type="html"><![CDATA[An introduction to fitting data and understanding the chi-squared statistic]]></summary></entry><entry><title type="html">Least-squares and fitting curves through points</title><link href="https://dvdjsp.github.io/blog/2026/LeastSq/" rel="alternate" type="text/html" title="Least-squares and fitting curves through points"/><published>2026-01-15T00:00:00+00:00</published><updated>2026-01-15T00:00:00+00:00</updated><id>https://dvdjsp.github.io/blog/2026/LeastSq</id><content type="html" xml:base="https://dvdjsp.github.io/blog/2026/LeastSq/"><![CDATA[<style>.question-box{border:2px solid #a463bf;border-radius:8px;padding:20px;margin-bottom:25px;background-color:#f9f9f9}.question-box h3{color:#a463bf;margin-top:0;font-size:1.3em;font-weight:700}.question-content{font-size:1.05em;line-height:1.6;color:#333;margin-bottom:15px}details{border:1px solid #ddd;border-radius:6px;margin-top:15px;background-color:#fff}details>summary{cursor:pointer;padding:12px 15px;background-color:#a463bf;color:white;font-weight:600;font-size:1.05em;border-radius:5px;transition:background-color .2s}details>summary:hover{background-color:#8b4ba3}details[open]>summary{border-radius:5px 5px 0 0;margin-bottom:15px}.solution-content{padding:15px;line-height:1.7;color:#333}.solution-content p{margin-bottom:10px}.page-header{text-align:center;margin-bottom:40px;padding-bottom:20px;border-bottom:3px solid #a463bf}.page-header h1{color:#a463bf;margin-bottom:10px}.page-header p{color:#666;font-size:1.1em}html[data-theme='dark'] .question-box{background-color:#1a1a1a;border-color:#2698ba}html[data-theme='dark'] .question-box h3{color:#2698ba}html[data-theme='dark'] .question-content{color:#e0e0e0}html[data-theme='dark'] details{background-color:#2a2a2a;border-color:#444}html[data-theme='dark'] details>summary{background-color:#2698ba}html[data-theme='dark'] details>summary:hover{background-color:#3bb5d9}html[data-theme='dark'] .solution-content{color:#e0e0e0}html[data-theme='dark'] .page-header h1{color:#2698ba}html[data-theme='dark'] .page-header p{color:#b0b0b0}</style> <p><em>Given a bunch of data, how can we find a <strong>representative</strong> curve that approximates the data?</em></p> <p>Let us understand this question through an example. Suppose that you‚Äôre out there trying to measure the resistance of a resistor by studying its Voltage vs Current using the relation \(V=I R\). Unfortunately, your equipment is not up to the standard you‚Äôd expect: it produces noisy data and you observe the following</p> <pre><code class="language-plotly">{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 11,
      "mode": "markers",
      "name": "Measured Data",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Voltage vs Current (Noisy Measurements)"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
</code></pre> <p><em>If only</em> this was a nice straight line, then using \(V=I R\), you can immediately extract the resistance by finding its slope. However, the real world is messy and chaotic. So instead of wishing for what could have been, lets try to work with what we have: <em>what if we could extract a straight line from the data?</em> A ‚Äúline of best fit‚Äù that passes through the points <strong>as close as possible</strong>. If we have that line, then the resistance is <em>approximately</em> the slope of that line. So a natural question to ask is <strong>how can one find this line?</strong></p> <p>Let‚Äôs denote this arbitrary line, i.e our <strong>model</strong> as a function \(f(x;m,c)\). A <strong>model</strong> is a function of your input variables (your control variables) and some parameters you want to estimate. Usually, a model is motivated by the physics and some underlying principles, but importantly it is something that you have to <em>declare</em>. In our case, the input variable is denoted by \(x\) (which is current here) and our parameters are the familiar <strong>parameters</strong> \(m\) (slope) and \(c\) (intercept) defining \(f(x; m,c)\) through</p> \[f(x;m,c)= m x +c.\] <p>The two parameters \((m,c)\) uniquely determines a line and therefore our special model is called a linear model. In principle, there are infinitely many values of \((m,c)\) and it is our goal to comb through this forest of \(m\) and \(c\) to find the correct \(m_0\) and \(c_0\) for which the line \(f(x;m_0,c_0)\) passes the ‚Äúclosest‚Äù to our data points. On that note, <em>What does ‚Äúclosest‚Äù even mean</em>?</p> <div class="question-box"> <h3 id="what-does-closest-mean">What does closest mean?</h3> <p>Suppose you‚Äôve collected \(N\) data points \(y_i\) for every input (control variable) value \(x_i\) where \(i=1,2,\dots,N\), then for every data point we can find \(c\) and \(m\) that minimizes the distance between the data point and your model that is given by \(\vert y_i - f(x_i; m,c)\vert\). This distance has a name, it is called the <strong>residue</strong>. We can mathematically write our process of minimizing the distance between our model and our data as over all the possible values of \(m\) and \(c\) as</p> \[\min_{m, c} \{\vert y_i-f(x_i;m ,c)\vert\} \text{ for all } i=1,2,\dots N.\] </div> <p>This is fine in theory but not in practice as you‚Äôd have to sample all the infinitely many values of \(m\) and \(c\) which can take forever. However, you remember from Calculus 1 that an optimization problem can be written in terms of a derivative (set to \(0\)), so you might wonder if there is a way to implement this in our problem.</p> <div class="question-box"> <h3 id="minimization-using-calculus">Minimization using Calculus</h3> <p>It turns out, you <strong>can</strong> rewrite this in terms of a function whose derivative set to \(0\) gives us the optimal solution, but we need to make a few compromises:</p> <ol> <li>Change \(\vert X \vert\) to \(X^2\), since the absolute value function is notoriously non-differentiable at \(X=0\) and</li> <li>Minimize the sum of the \(\text{distance}^2\) of all the data points instead of minimizing the distance individually.</li> </ol> <p>(1) and (2) together is equivalent to minimizing the following <strong>objective function</strong> \(\mathcal{L}\) over all possible values of \(m\) and \(c\)</p> \[\mathcal{L}(m,c)=\sum_{i=1}^{N} [y_i- f(x_i; m,c)]^2.\] <p>The above formalism is called the <a href="https://en.wikipedia.org/wiki/Least_squares">least-squares method</a>, since you want to minimize the (sum) squared distance of the measured data points and the model.</p> </div> <p>In our case, minimizing the least squares (our objective function here) is the same as finding solutions \((m,c)\) where the derivatives vanish.</p> <div class="question-box"> <h3 id="what-kind-of-derivatives">What kind of derivatives?</h3> <p>If our model only had one parameter \(\theta\), then it would be an ordinary derivative \(\frac{d}{d\theta}\). However, since we‚Äôve got two parameters, we need to consider the <strong>partial</strong> derivatives in both, i.e we solve</p> \[\begin{align} \frac{\partial}{\partial m} \mathcal{L}(m,c)&amp;=0\\ \frac{\partial}{\partial c} \mathcal{L}(m,c)&amp;=0 \end{align}\] <p>Solving for both simultaneously, we‚Äôll be able to find the optimum solution \(m_0\) and \(c_0\) for which our line \(f(x;m_0,c_0)=m_0 x +c_0\) is the closet to our observed data. In the special case of a linear model , we can find the unique solution exactly (try working it out!).</p> </div> <p>Usually these calculations are done at the blink of an eye through existing packages like <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html"><code class="language-plaintext highlighter-rouge">curve_fit()</code></a> from <code class="language-plaintext highlighter-rouge">scipy.optimize</code> in <code class="language-plaintext highlighter-rouge">python</code>. Here‚Äôs what the least squares fit looks like for our noisy data:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 15,
      "mode": "markers",
      "name": "Measured Data",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    },
    {
      "fitFromTrace": 0,
      "points": 100,
      "mode": "lines",
      "name": "Least Squares Fit",
      "line": {
        "color": "#000101ff",
        "width": 3,
        "dash": "solid"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Voltage vs Current with Least Squares Fit"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
</code></pre> <div class="question-box"> <h3 id="what-lies-beyond-the-linear-model">What lies beyond the linear model?</h3> <p>After dabbling with the simple linear model, we may wonder what lies beyond it: What if we‚Äôd like to fit a parabola through your data points? What if an inverse square? For cases when linearization is possible, <strong>always linearize</strong>, i.e if your model was \(f(x; a,b)= a x^2 +b\), then set \(X=x^2\) and then you have a linear model \(g(X; a,b)= a X +b\). However, in cases where such a linearization is not possible and your model is something like \(a x^2 +b x +c\) then you continue systematically but solve the set of equations</p> \[\begin{align} \frac{\partial}{\partial a} \mathcal{L}(a,b,c)&amp;=0\\ \frac{\partial}{\partial b} \mathcal{L}(a,b,c)&amp;=0\\ \frac{\partial}{\partial c} \mathcal{L}(a,b,c)&amp;=0 \end{align}\] <details style="padding: 15px 20px;"> <summary>Model with multiple parameters and multiple inputs</summary> <p>If you had a general model with \(M\) parameters \(\theta_1,\theta_2,\dots,\theta_M\) that takes in \(P\) inputs \(\boldsymbol{x}=(x_1,x_2,\dots, x_P)\) given by \(f(\boldsymbol{x}; \boldsymbol{\theta})\) that we can gather as an array or vector commonly denoted with a bold \(\boldsymbol{\theta}\)</p> \[\boldsymbol{\theta}=(\theta_1,\theta_2,\dots \theta_M),\] <p>Then we solve the set of equations</p> \[\begin{align} \frac{\partial}{\partial \theta_1} \mathcal{L}(\boldsymbol{\theta})&amp;=0\\ \frac{\partial}{\partial \theta_2} \mathcal{L}(\boldsymbol{\theta})&amp;=0\\ \vdots &amp; =0 \\ \frac{\partial}{\partial \theta_M} \mathcal{L}(\boldsymbol{\theta})&amp;=0\\ \end{align}\] <p>for the optimum solution \(\boldsymbol{\theta}_0\) which minimizes our objective function defined by</p> \[\mathcal{L}(\boldsymbol{\theta})= \sum_{i=1}^{N} [y_i - f(\boldsymbol{x}_i; \boldsymbol{\theta})]^2.\] <p>Our optimal solution \(\boldsymbol{\theta}_0\) brings our model \(f(\boldsymbol{x}; \boldsymbol{\theta}_0)\) the closest to our data points.</p> </details> </div> <p>Now you‚Äôve figured out how to fit data and you happily move on with your life, succeeding in extracting the resistance until your friend comes along and performs the same experiment and observes the following</p> <pre><code class="language-plotly">{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 11,
      "mode": "markers",
      "name": "Measured Data",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Voltage vs Current (Friend's)"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
</code></pre> <p>You notice that their data is <strong>not</strong> exactly the same as yours (look above!). As special as your friend is to you, their data isn‚Äôt. Indeed, anyone perform the same set of measurement (called a trial) and get different data \(y_i\). At this stage, you realize that your data set is one out of the infinity many <em>possible</em> realizations, so it is natural to somehow factor that <em>uncertainty</em> in your measurement.</p> <p>How do we do that you say? Through the \(\chi^2\) test that you can read about here: <a href="/blog/2026/Chisq/">What the heck is \(\chi^2\) anyway?</a></p>]]></content><author><name></name></author><category term="cool-math-stuff"/><category term="DataFitting"/><category term="Optimization"/><category term="Probability"/><summary type="html"><![CDATA[An introduction to the least squares method]]></summary></entry><entry><title type="html">Guess The Date!</title><link href="https://dvdjsp.github.io/blog/2024/Shannon/" rel="alternate" type="text/html" title="Guess The Date!"/><published>2024-11-29T00:00:00+00:00</published><updated>2024-11-29T00:00:00+00:00</updated><id>https://dvdjsp.github.io/blog/2024/Shannon</id><content type="html" xml:base="https://dvdjsp.github.io/blog/2024/Shannon/"><![CDATA[<p>Need to write it out here :) üõ†Ô∏è</p>]]></content><author><name></name></author><category term="cool-math-stuff"/><category term="InformationTheory"/><category term="ShannonEntropy"/><category term="Probability"/><summary type="html"><![CDATA[An introduction to Shannon Entropy and quantifying information.]]></summary></entry></feed>