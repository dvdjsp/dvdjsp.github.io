<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://dvdjsp.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://dvdjsp.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-26T19:26:53+00:00</updated><id>https://dvdjsp.github.io/feed.xml</id><title type="html">Davidson Noby Joseph</title><subtitle>A Curious Grad Student in Physics </subtitle><entry><title type="html">What the heck is \(\chi^2\) anyway?</title><link href="https://dvdjsp.github.io/blog/2026/Chisq/" rel="alternate" type="text/html" title="What the heck is \(\chi^2\) anyway?"/><published>2026-01-16T00:00:00+00:00</published><updated>2026-01-16T00:00:00+00:00</updated><id>https://dvdjsp.github.io/blog/2026/Chisq</id><content type="html" xml:base="https://dvdjsp.github.io/blog/2026/Chisq/"><![CDATA[<p><em>How do you fit a curve through data with errors? What’s a statistic? and most importantly <strong>what is being squared?</strong></em></p> <p>These are important questions that you might encounter when you look up curve fitting! This is a continuation of <em>how to fit data</em> where I answer the former in <a href="/blog/2026/LeastSq/">Least-squares and fitting curves through points</a> when the data is fixed. Before getting into what a statistic is, let’s first begin by answering the most common and important question here:</p> <style>
.question-box {
  border: 2px solid #a463bf;
  border-radius: 8px;
  padding: 20px;
  margin-bottom: 25px;
  background-color: #f9f9f9;
}

.question-box h3 {
  color: #a463bf;
  margin-top: 0;
  font-size: 1.3em;
  font-weight: 700;
}

.question-content {
  font-size: 1.05em;
  line-height: 1.6;
  color: #333;
  margin-bottom: 15px;
}

details {
  border: none;
  border-radius: 6px;
  margin-top: 15px;
  margin-bottom: 25px;
  background-color: transparent;
}

details[open] {
  border: 1px solid #ddd;
  background-color: #fff;
}

details > summary {
  cursor: pointer;
  padding: 12px 15px;
  background-color: #a463bf;
  color: white;
  font-weight: 600;
  font-size: 1.05em;
  border-radius: 5px;
  transition: background-color 0.2s;
}

details > summary:hover {
  background-color: #8b4ba3;
}

details[open] > summary {
  border-radius: 5px 5px 0 0;
  margin-bottom: 15px;
}

.solution-content {
  padding: 15px;
  line-height: 1.7;
  color: #333;
}

.solution-content p {
  margin-bottom: 10px;
}

.page-header {
  text-align: center;
  margin-bottom: 40px;
  padding-bottom: 20px;
  border-bottom: 3px solid #a463bf;
}

.page-header h1 {
  color: #a463bf;
  margin-bottom: 10px;
}

.page-header p {
  color: #666;
  font-size: 1.1em;
}

/* Dark mode styles */
html[data-theme='dark'] .question-box {
  background-color: #1a1a1a;
  border-color: #2698ba;
}

html[data-theme='dark'] .question-box h3 {
  color: #2698ba;
}

html[data-theme='dark'] .question-content {
  color: #e0e0e0;
}

html[data-theme='dark'] details[open] {
  background-color: #2a2a2a;
  border-color: #444;
}

html[data-theme='dark'] details > summary {
  background-color: #2698ba;
}

html[data-theme='dark'] details > summary:hover {
  background-color: #3bb5d9;
}

html[data-theme='dark'] .solution-content {
  color: #e0e0e0;
}

html[data-theme='dark'] .page-header h1 {
  color: #2698ba;
}

html[data-theme='dark'] .page-header p {
  color: #b0b0b0;
}
/* Remark box styles */
.remark-box {
  border: 2px solid #e57373;
  border-radius: 8px;
  padding: 20px;
  margin-bottom: 25px;
  background-color: #ffebee;
}

.remark-box h3 {
  color: #d32f2f;
  margin-top: 0;
  font-size: 1.3em;
  font-weight: 700;
}

.remark-content {
  font-size: 1.05em;
  line-height: 1.6;
  color: #333;
}

/* Dark mode styles for remark box */
html[data-theme='dark'] .remark-box {
  background-color: #1a1a1a;
  border-color: #4db6ac;
  background-color: #0d2a28;
}

html[data-theme='dark'] .remark-box h3 {
  color: #4db6ac;
}

html[data-theme='dark'] .remark-content {
  color: #e0e0e0;
}

/* Example box styles */
.example-box {
  border: 2px solid #66bb6a;
  border-radius: 8px;
  padding: 20px;
  margin-bottom: 25px;
  background-color: #e8f5e9;
}

.example-box h3 {
  color: #2e7d32;
  margin-top: 0;
  font-size: 1.3em;
  font-weight: 700;
}

.example-content {
  font-size: 1.05em;
  line-height: 1.6;
  color: #333;
}

/* Dark mode styles for example box */
html[data-theme='dark'] .example-box {
  background-color: #1a1a1a;
  border-color: #ab47bc;
  background-color: #2a1a2d;
}

html[data-theme='dark'] .example-box h3 {
  color: #ab47bc;
}

html[data-theme='dark'] .example-content {
  color: #e0e0e0;
}
</style> <div class="question-box"> <h3 id="what-is-being-squared">What is being squared?</h3> <p><strong>Nothing!</strong> This is a trick question used by cruel people. Unfortunately, this is a great example of an abuse of notation: \(\chi^2\) is a variable, just like \(x\) or \(y\). We could have called it \(\chi\), but for historical reasons, <a href="https://doi.org/10.1080/14786440009463897">Pearson called it \(\chi^2\).</a> probably to indicate that this quantity is \(\geq 0\).</p> </div> <p>Now that we’ve tackled this important question, let us motivate the statistic with the same example we used previously:</p> <p>You (and your friend) are trying to measure the resistance of a resistor by studying its Voltage vs Current using the relation \(V=I R\). Unfortunately, your equipment isn’t the greatest and you get noisy data that is “roughly” linear. Adding to that, neither of your sets of measurements are identical, i.e not only are they not on a line, they are different for the both of you!</p> <pre><code class="language-plotly">{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 15,
      "mode": "markers",
      "name": "Trial 1",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "First Measurement"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
</code></pre> <pre><code class="language-plotly">{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 15,
      "mode": "markers",
      "name": "Trial 2",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Second Measurement"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
</code></pre> <p>It seems that there isn’t <em>just</em> an uncertainty that prevents the data from landing perfectly on a line, but also in addition, each data point <strong>has uncertainties</strong></p> <p>Earlier we laid out the ideas behind curve fitting, but now we can now answer the more physical question of fitting data <strong>with uncertainties</strong> which is where \(\chi^2\) comes into play. The \(\chi^2\) statistic (pronounced <strong>chi-squared</strong>) is a metric used to asses a fit of a collection of data with <strong>errors</strong>. The introduction of errors changes the way we think of the optimization problem! Keep in mind the following remark before we proceed:</p> <div class="remark-box"> <h3 id="remark">Remark</h3> <div class="remark-content"> <p>Unlike least squares which was purely an optimization problem that fits curves to <strong>a</strong> given set of data, \(\chi^2\) fitting additionally accounts for the errors the data carries. Basically, \(\chi^2\) fitting is an optimization problem with statistics.</p> </div> </div> <p>To work with \(\chi^2\), we first need some important ideas from statistics so we’ll start here. If you want to skip to using it, then click <a href="#how-chi2-goodness-of-fit-is-used">how to use \(\chi^2\) goodness of fit</a>.</p> <h1 id="some-statistics-overview"><span style="color: #a463bf;">Some statistics overview</span></h1> <div class="question-box"> <h3 id="dr-strange-and-the-very-many-different-outcomes">Dr. Strange and the very many different outcomes</h3> <p>You put on your thinking cap and recognize that each data point you measure is really one possible outcome that you experience. Like Dr. Strange using his ability to view various possible multiverses, you view each data point you see as a specific outcome, and the collection of all possible outcomes together is what you would get if you repeat the experiment infinitely many times. In this sense, we say that the the data points are <strong>sampled from a distribution</strong> which controls the probability of each measurement showing up as a specific value. Mathematically, what this means is that all the data points \(y_i\) are <strong>random variables</strong>.</p> <p>What this implies is that the more data you measure, the more information you get about each data point and therefore the more accurate your results become. The true values of each data point should be approximately the mean of all the data you have for that particular data point. The variation is its standard error of the mean and you plot them together with error bars.</p> </div> <p>To get an intuitive feel for what this means, lets think about it in terms of an excel sheet. If you imagine an excel sheet keeping track of your measured data with each row corresponding to its trial number and each column being the measured values for the \(i^{\rm th }\) data point \(y_i\), then isolating a column (say the first column of \(y_1\)) over <u>all trials</u> gives you the different possible values that data point can \(y_i\) take. How do we use this information? We average!</p> <p>The idea is to collapse the information of multiple plots by aggregating a specific column and plotting the corresponding averages as data points \(\bar{y}_i\)</p> <pre><code class="language-plotly">{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 15,
      "mode": "markers",
      "name": "Data ",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      },
      "error_y": {
        "type": "constant",
        "value": 1.0,
        "visible": true,
        "color": "#a463bf",
        "thickness": 2,
        "width": 4
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Voltage vs Current with Error Bars"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
</code></pre> <p>where we <strong>call</strong> the error bars of size \(\sigma_{\bar{y}_i}\) as the <strong>standard error of the mean</strong> for each column.</p> <details style="padding: 15px 20px;"> <summary>Population and sample data sets, standard errors and the truth!</summary> <p>In statistics, a lot of similar sounding terms can be thrown around that can lead to confusion. Here I would like to clarify the difference between population and sample data set. The quantities “population mean”, “sample standard deviation” etc are known as a measured <strong>statistic</strong>.</p> <p>In a nutshell, the population data set is the <u>true data set</u>, everyone and everything that you seek to study is present in the data set. Its mean (called the <em>population mean</em> denoted by \(\mu\)) and standard deviation (called the <em>population standard deviation</em> denoted by \(\sigma\)) is the <strong>true</strong> or correct statistic, i.e it is the underlying truth. However, in real life, it’s often not possible to get the whole collection, so we work with a small subset called the sample data set, i.e <u>your</u> data set. Ultimately, everything we do and measure is to estimate the true statistic. Keep in mind:</p> <ul> <li> <p>The sample mean \(\bar{s}\) corresponds to the mean of your column, it is what we call an “estimator” of the true population mean</p> </li> <li> <p>The sample standard deviation \(s\) is the standard deviation of your column computed using <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction">Bessel’s correction</a>, it is also an “estimator” of the true population standard deviation.</p> </li> <li> <p>The sample standard error, which is the standard deviation of the mean given by</p> \[\sigma_{\bar{s}}= \frac{s}{\sqrt{N}}.\] </li> </ul> <p>The sample mean and standard deviation is pretty intuitive, however the standard error is often misunderstood due to its confusing notation. Measuring a bunch of noisy data gives you an average number which is known as the sample mean. The spread of the data is characterized by the sample standard deviation. When computing averages, you might have noticed that the averages for different data sets are <em>“close”</em> but <u> not the same</u>, i.e the averages themselves also spread. This spread, the spread of the means from <em>different</em> data sets is called the <strong>sample standard error</strong>, and is given by</p> \[\sigma_{\mu}= \frac{\mu}{\sqrt{N}}\] <p>if you know the true population mean \(\mu\), but we approximate it by the sample standard error \(\sigma_{\bar{s}}\).</p> </details> <p>Now that we know of a method to encode the information from multiple trials through aggregating, we notice that for different data sets, we get different means \(\bar{y}_i\), i.e the means are “random”. This means that <u>the means</u> themselves are random variables. So we ask:</p> <div class="question-box"> <h3 id="what-distribution-does-the-data-point-baryi-come-from">What distribution does the data point \(\bar{y}_i\) come from?</h3> <p>This question is a genuinely hard question that is difficult to answer with absolute truth. The simplest answer is to make an educated guess. However, <strong>given certain reasonable conditions</strong> something truly spectacular happens: you can assume that the data point \(\bar{y}_i\) comes from a <strong>Normal distribution</strong> \(\mathcal{N}(\mu_i,\sigma_i)\) through something called the <strong>Central Limit Theorem</strong>.</p> <p>Before we venture into this, let’s think about the weight of this conclusion. Out of the many distributions you know <em>viz.</em> the humble <a href="">uniform distribution</a>, or the <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a> (discrete) or <a href="https://en.wikipedia.org/wiki/Exponential_distribution">Exponential distribution</a> (continuous) or the many many more distributions you most likely haven’t heard of<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">1</a></sup> like the <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a> or the <a href="https://en.wikipedia.org/wiki/Continuous_Bernoulli_distribution">Continuous Bernoulli Distribution</a>, we arrive at the seemingly innocent Normal distribution!</p> <p>This Normal distribution or the “bell shaped curve” with mean \(\mu\) and variance \(\sigma^2\) is given by the following equation</p> \[\mathcal{N}(\mu,\sigma^2)(x):= \frac{1}{\sqrt{2\sigma^2 \pi}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}\] <p>and is the renowned “Gaussian distribution” but normalized</p> \[\int_{-\infty}^{\infty} \mathcal{N}(\mu,\sigma^2)(x)dx=1.\] <p>The equation for this distribution describes the <em>probability density</em> of observing a value \(x\). If a random variable is governed by the normal distribution, then the <strong>probability</strong> that this variable takes a value between two numbers \(a&lt;b\) is exactly the area under the curve!</p> <p>The Central Limit Theorem implies that the sample mean we were measuring \(\bar{y}_i\) using \(N\) trials comes from a normal distribution represented mathematically as</p> \[\bar{y}_i \sim \mathcal{N}\left(\mu_i,\frac{\sigma_i^2}{N}\right),\] <p>where \(\mu_i\) is the true population mean and \(\sigma_i\). Mathematically, this means that our measured sample mean behaves (follows) a normal distribution with a mean that is the true population mean and standard deviation which is the standard error of \(\mu_i\). In the voltage vs current scenario, the population mean is the “true” reading of voltage at a fixed current, and the standard deviation describes the fluctuation width around that value. In practice, the population mean and standard deviation are unknown but we can approximate them using our data. Taking more and more data, we can take averages and if the errors are Gaussian, then average of our voltages for a fixed current converges to a number and its width, the standard error becomes small.</p> <p><u>For our fitting purposes</u> however, we only estimate the standard deviation of each data point since the “mean” for our data points is what we want our optimization to pick out.</p> <details style="padding: 15px 20px;"> <summary>Central Limit Theorem: Why <u>the</u> Normal distribution?</summary> <p>The <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem (CLT)</a> is what we used to justify the emergence of the Normal distribution. This theorem is colloquially stated along the lines of <em>“more data means more bell shaped”</em> but this isn’t quite the correct statement of the theorem and can lead to a lot of misunderstandings. There are a few conditions for the CLT to hold, and they are nicely discussed in <a href="https://www.youtube.com/watch?v=zeJD6dqJ5lo">3B1B’s “But what is the Central Limit Theorem?”</a> that I will paraphrase below:</p> <ol> <li> <p><strong>Independence</strong>: The data you collect should be independent of each other, i.e measuring one does not affect the next measurement.</p> </li> <li> <p><strong>Same distribution</strong>: The data you collect, i.e the data points in the columns in your excel sheet should come from the same distribution.</p> </li> <li> <p><strong>Finite variance</strong>: Your error bars must be finite, i.e more data does not increase the variance you observe for a single column.</p> </li> </ol> <p>Denote by \(y_i^{(j)}\) the data points in the \(i^{\rm th}\) column and \(j^{\rm th}\) row. Given the above conditions, CLT states that in the limit as you perform many many many trials (i.e the number of rows \(N\) goes to infinity),</p> <p>The ratio of the difference between the quantity</p> \[\lim_{N\to \infty} \frac{\sum_{j=1}^N y_i^{(j)}}{N}\] <p>which is just the average of our column with the true population mean over the population standard error follows</p> \[\lim_{N\to \infty} \frac{\frac{\sum_{j=1}^N y_i^{(j)}}{N}- \mu_i}{\frac{\sigma_i}{\sqrt{N}}}\sim \mathcal{N}(0,1)\] <p>i.e a Normal distribution with mean zero and standard deviation one!</p> <p>This means the average of a fixed column in the limit of more data points follows a normal distribution around the population mean \(\mu_i\) and variance which is the square of the standard error of the mean:</p> \[\lim_{N\to \infty} \frac{\sum_{j=1}^N y_i^{(j)}}{N}\sim \mathcal{N}\left(\mu_i,\frac{\sigma_i^2}{N}\right)=\mathcal{N}\left(\mu_i,\sigma^2_{\mu}\right)\] </details> </div> <p>All that jazz just to say that for our fitting purposes, we need to assume (with reasonable doubt) that the errors we measure or estimate are Gaussian errors, and of course, that we need to account for them.</p> <p>Since we’re right at the intersection of pure mathematics where everything is “perfect” and experimental data where nature is unwieldy, I want to caution you to always challenge your beliefs with a relevant quote<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> from the French physicist <a href="https://en.wikipedia.org/wiki/Gabriel_Lippmann">G. Lippmann</a> (Nobel Prize 1908) paraphrased by the prominent mathematician <a href="https://en.wikipedia.org/wiki/Henri_Poincar%C3%A9">H. Poincaré</a>:</p> <blockquote> <p>“Tout le monde y croit cependant, me disait un jour M. Lippmann, car les expérimentateurs s’imaginent que c’est un théorème de mathématiques, et les mathématiciens que c’est un fait expérimental.”</p> <p><br/></p> <p>“Everyone is sure of this [that errors are normally distributed], Mr. Lippman told me one day, since the experimentalists believe that it is a mathematical theorem, and the mathematicians that it is an experimentally determined fact.” -G Lippmann to H. Poincaré (<a href="https://www.ime.usp.br/~walterfm/cursos/mac5796/Poincare12.pdf">Calcul des probabilités 1912, p. 171</a>)</p> </blockquote> <h1 id="how-chi2-goodness-of-fit-is-used"><span style="color: #a463bf;" id="how-chi2-goodness-of-fit-is-used">How \(\chi^2\) goodness of fit is used</span></h1> <p>But <em>how</em> do we account for these errors in our fit? If we didn’t have errors for our data point, then we’d stick to our usual least squares method and minimize the least squares. Essentially, we’re trying to make the distance between our model \(f(x;\boldsymbol{\theta})\) and our data points \(y_i\) small, but with errors, <em>what does small even mean now?, i.e small with respect to <strong>what</strong>?</em></p> <p>In the case where data points have errors, a natural “scale” we’re trying to compare with is against the fluctuations–the error \(\sigma_i\) per data point. The error is estimated using the sample standard deviation from multiple trials for a data point or guesstimated based on the order of magnitude. With this scale in mind, we minimize this sort of modified or scaled least squares which is what we call \(\chi^2\) defined as</p> \[\begin{equation}\chi^2 :=\sum_{i=1}^N \left(\frac{y_i- f(x;\boldsymbol{\theta})}{\sigma_i}\right)^2.\end{equation}\] <p>If we’re fitting a <strong>line</strong> to our data points with errors (as in the example of \(V\) vs \(I\)), then we’d use \(f(x;m,c):=m x +c\) for the free parameters of the line \(m\) (slope) and \(c\) (intercept) where \(\boldsymbol{\theta}=(m,c)\). Since errors \(\sigma_i\) are just (estimated or guesstimated) and comes in as a multiplicative factor to residues, the <u>procedure</u> to find the optimal solution is identical to what we did in <a href="/blog/2026/LeastSq/">least squares and fitting curves through points</a>. Since \(\chi^2\) is just “least squares with errors for each data point”, we’d still use <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html"><code class="language-plaintext highlighter-rouge">scipy.optimize.curve_fit()</code></a> in <code class="language-plaintext highlighter-rouge">python</code> with <code class="language-plaintext highlighter-rouge">sigma</code> (make sure to set <code class="language-plaintext highlighter-rouge">absolute_sigma=True</code>). Earlier, the smaller our least squares, the better fit our data is to our model, therefore we may naturally think that smaller \(\chi^2\) means better results; the question now is if this assumption still holds.</p> <p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">scipy.optimize.curve_fit()</code> does not return \(\chi^2\), it returns the parameters of your model i.e your fit. You’d have to compute \(\chi^2\) by using those parameters in Eq. (1)</p> <div class="question-box"> <h3 id="reduced-chi2-and-what-it-means-to-be-a-good-fit">Reduced \(\chi^2\) and what it means to be a “good” fit</h3> <p>\(\chi^2\) by itself for a particular fit does not tell us much: <em>should it be very small? should it be very big?</em> A common (but understandable) misconception is that since \(\chi^2\) comes from least squares, obtaining the smallest value of \(\chi^2\) should correspond to a good fit since that worked for least squares. However, this is <u>not</u> true! The quantity you need to check against is the so called <strong>reduced chi-squared</strong> defined by</p> \[\chi^2_{\nu} := \frac{\chi^2}{\nu}\] <p>where</p> \[\begin{equation}\nu=N-p\end{equation}\] <p>is called the <strong>degrees of freedom</strong> that you can generally find by subtracting the number of data points with the number of <u>independent</u> parameters you used for the model. For example: If you had 10 data points that you’re trying to fit a line to, then \(\nu=10-2=8\). The degrees of freedom is crucial to our fit statistic since that is what allows us to decide how good our fit is after evaluating the fit’s \(\chi^2\).</p> <p>What value of \(\chi^2_{\nu}\) is good? It turns out that the expected value (average denoted by \(\mathbb{E}\)) of \(\chi^2_{\nu}\) for a good fit is unity, i.e</p> \[\mathbb{E}[\chi^2_{\nu}] \approx 1\] <p>(with equality for a linear model in the parameters) meaning that a fit is “good” if</p> \[\chi^2_{\nu} \approx 1\] <p>We will talk about why this is what we should expect under certain conditions later down the section.</p> </div> <p>Now that we have answered how to use and implement \(\chi^2\), we must address the elephant in the room: <em>when</em> do we use it?</p> <h1 id="when-chi2-goodness-of-fit-is-used"><span style="color: #a463bf;" id="how-chi2-goodness-of-fit-is-used">When \(\chi^2\) goodness of fit is used</span></h1> <p>From what we did above, it might seem like \(\chi^2\) is a very well defined and ready to use statistic, however it turns out that it is a not a universally good fit statistic! More precisely, our assumption that a model \(f\) with \(p\) parameters and \(N\) data points having \(N-p\) degrees of freedom is not true and therefore our comparison of \(\chi^2_\nu\approx 1\) would be void.</p> <div class="example-box"> <h3 id="example-independent-parameters">Example: Independent parameters?</h3> <div class="example-content"> <p>Consider the following simple three parameter model</p> \[f(x;A,B,C)= A+ (B+C) x\] <p>After fitting our parameters and evaluating \(\chi^2\) at those parameters, we evaluate the degrees of freedom \(\nu=N-3\) to eventually check the value of \(\chi^2_{\nu}\) and draw our conclusions from it. But wait! <em>Is p=3</em>? <em>Does our model really have three <u>independent</u> parameters?</em> Can your fit really separate \(B\) from \(C\) from the data? How could they even? It turns out that we can’t: we say that \(B\) and \(C\) are not independent from each other. The maximum number of independent parameters for this kind of model is two, and is given by \(f(x;M,C)=C+ M x\). In hindsight, we <em>intuitively</em> know that this model should only have two parameters, it’s just the general equation of a line!</p> </div> </div> <p>Even if we’re careful with the problem of independent parameters, we are still not <strong>guaranteed</strong> \(\nu=N-p\). For instance, <em>Andrae et. al</em> in <a href="https://arxiv.org/abs/1012.3754">Do’s and dont’s of reduced chi-squared [Eq. (10)]</a> considers a pathological example that is particular illuminating<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">3</a></sup></p> <div class="example-box"> <h3 id="example-fitting-to-noise">Example: Fitting to noise</h3> <p>Consider the three parameter model</p> \[f(x; A,B,C)= A \cos(Bx+C).\] <p>It turns out that this model is capable of fitting <u>any data</u> given that no two data points have identical \(x\) values. Have a linear fit? use Eq. (10). Have an exponential fit? use Eq. (10) … see what I mean? Why is that intuitively so you ask? because given any data, we can tune the amplitude to capture all the points that range over a fixed y value and then increase the frequency to densely wiggle near them. This means that the \(\chi^2\) can be arbitrary close to zero if you let the optimizer search for unbounded \(A,B,C\).</p> </div> <p>These examples illustrate the common pitfalls of \(\chi^2\), and is a cautionary tale in our quest to fitting data. However, in most cases, (polynomial fit, exponential, Gaussian &amp; Lorentzian), under most reasonable circumstances (viz. the parameters are finite and sometimes non-zero), \(\chi^2\) is a pretty reasonable and useful fit statistic. I hope that this adventure has taught you a great deal about fitting and given you a chance to appreciate the nuances behind curve fitting. I will conclude this article with an optional section on why \(\chi^2_\nu\approx N-p\).</p> <details style="padding: 15px 20px;"> <summary>Why do we expect \(\chi^2_{\nu}\approx 1\)?</summary> <p>The question can be answered in two parts. For a general non-linear model with \(p\) independent parameters that behaves nicely (i.e avoids the issues of \(\chi^2\)), we can Taylor expand the model around the <strong>truth</strong> denoted by \(\boldsymbol{\theta}_0\) and we get a linear model. If we assume that our model is well behaved, then our solution from the optimization \(\boldsymbol{\hat{\theta}}\) should be <em>close</em> to the truth and therefore it makes sense to expand about the truth after fitting to linear order since \(\vert\vert \boldsymbol{\theta}_0^2-\hat{\theta}\vert\vert^2\approx 0\). For this local linear model, if we show that \(\mathbb{E}[\chi^2_{\nu}]=N-p\), then we should expect our non-linear model to have \(\mathbb{E}[\chi^2]\approx N-p\) <strong>given</strong> that \(\boldsymbol{\hat{\theta}}\approx \boldsymbol{\theta}_0\).</p> <p>Given these conditions, it is sufficient to work with a general linear model in the \(p\) parameters</p> \[f(x;\boldsymbol{\theta})= \boldsymbol{a(x_i)}\cdot \boldsymbol{\theta}=a_1(x) \theta_1+ a_2(x) \theta_2+\dots+a_p(x)\theta_p,\] <p>where \(a_j(x)\) are arbitrarily nice functions of \(x\) but maybe different for different \(j\).</p> <p>Consider now the residual vector with components \(i=1,\dots,N\) where \(N\) is the number of data points you collect, then \(\chi^2\) can be written in terms of the residual vector \(\boldsymbol{r}\) as</p> \[\chi^2(\boldsymbol{\theta})=\boldsymbol{r}(\boldsymbol{\theta})\cdot \boldsymbol{r}(\boldsymbol{\theta}),\] <p>with components</p> \[r_i= \frac{y_i- f(x_i,\boldsymbol{\theta})}{\sigma_i}.\] <p>Since the components are summed over \(a_j(x_i)\) for each \(i\), we express the whole vector \(\boldsymbol{r}\) in terms of a rectangular matrix \(A\) of size \(N\times p\) as</p> \[\boldsymbol{r}(\boldsymbol{\theta})= \boldsymbol{z}-A \boldsymbol{\theta}\] <p>where</p> \[\begin{align} z_i&amp;=\frac{y_i}{\sigma_i},\\ A_{ij}&amp;= \frac{a_j(x_i)}{\sigma_i}. \end{align}\] <p>Therefore, minimizing \(\chi^2\) is equivalent to the minimization problem</p> \[\chi^2(\boldsymbol{\theta})= (\boldsymbol{z}-A \boldsymbol{\theta})\cdot(\boldsymbol{z}-A \boldsymbol{\theta})= \vert\vert \boldsymbol{z}-A\boldsymbol{\theta}\vert\vert^2.\] <p>How do we minimize this? <strong>use calculus!</strong> Taking the partial derivatives of \(\theta_i\) for all of the \(p\) parameters and setting it to \(0\), we have</p> \[0=-2 A^T(\boldsymbol{z}-A \boldsymbol{\theta})\] <p>which implies that the optimal solution is given explicitly as</p> \[\boldsymbol{\hat{\theta}}= A (A^T A)^{-1} A^T\] <p>if \(A^T A\) which is an \(N\times N\) matrix is invertible. Plugging this back into our solution, we see the minimum value of \(\chi^2\) for a fixed data set using the optimal solution</p> \[\chi^2(\hat{\boldsymbol{\theta}})= \boldsymbol{z}^T(I_N-A (A^T A)^{-1} A^T)^T(I_N-A (A^T A)^{-1} A^T)\boldsymbol{z}=\boldsymbol{z}^T(I_N-A (A^T A)^{-1} A^T)\boldsymbol{z}.\] <p>Notice that the right hand side does not explicitly depend on \(\boldsymbol{\hat{\theta}}\) but rather the matrix \(A\) which is constructed from your data set. Now we imagine fitting this to another data set, and because it’s a different data set, we’d get a different \(\boldsymbol{\hat{\theta}^\prime}\) and so on and so forth. Taking all those data sets, and averaging the minimal \(\chi^2\) from all of them is equivalent to taking the expectation value \(\mathbb{E}[\chi^2]\) written as</p> \[\mathbb{E}[\chi^2]= \mathbb{E}[\boldsymbol{z}^T(I_N-A (A^T A)^{-1} A^T)\boldsymbol{z}].\] <p>What now? It would seem like we’re stuck since we’ve got no information about \(\boldsymbol{z}\). But fret not! It turns out that then the data points we collect are <strong>defined</strong> to fluctuate as</p> \[z_i=\frac{y_i}{\sigma_i}= \frac{f(x_i;\boldsymbol{\theta}_0)}{\sigma_i}+ \epsilon_i\] <p>where \(\epsilon_i\sim \mathcal{N}(0,1)\) which follows from our assumption that the errors are Gaussian. In our linear case,</p> \[\frac{f(x_i;\boldsymbol{\theta}_0)}{\sigma_i}=(A\boldsymbol{\theta})_i.\] <p>In vector form,</p> \[\boldsymbol{z}= A\boldsymbol{\theta}+ \boldsymbol{\epsilon}\] <p>where \(\boldsymbol{\epsilon}\) is a “random” vector with components centered around \(0\).</p> <p>Thus, our \(\chi^2\) is written as</p> \[\chi^2=\boldsymbol{z}^T(I_N-A (A^T A)^{-1} A^T)\boldsymbol{z}=\boldsymbol{\epsilon}^T(I_N-A (A^T A)^{-1} A^T)\boldsymbol{\epsilon}\] <p>Now let’s do some mathmagic! Using the definition of the product \(\boldsymbol{z}^TM\boldsymbol{z}\), we expand \(\boldsymbol{\epsilon}^T M \boldsymbol{\epsilon}\) to get</p> \[\mathbb{E}[\boldsymbol{\epsilon}^T M \boldsymbol{\epsilon}] = \mathbb{E}\left[\sum_{i,j=1}^N M_{ij}\epsilon_i\epsilon_j\right].\] <p>We use the linearity of the expectation value and the key property that the expectation value of the product of two different gaussian random variables from the same distribution is zero but the variable squared is 1 to get</p> \[\mathbb{E}[\epsilon_i \epsilon_j]= \begin{cases}1 &amp; \text{if } i=j,\\ 0 &amp; \text{if } i\neq j.\end{cases}\] <p>which gives</p> \[\mathbb{E}\left[\sum_{i,j=1}^N M_{ij}\epsilon_i\epsilon_j\right]=\sum_{i=1}^NM_{ii}:= {\rm tr}(M),\] <p>i.e the <strong>trace</strong> of a matrix. Using the linearity of the trace and the cyclic property</p> \[{\rm tr}(A+ B (DB)^{-1} D)= {\rm tr}(A) + {\rm tr}( DB (DB)^{-1}),\] <p>we have</p> \[\mathbb{E}[\chi^2]=\mathbb{E}[\boldsymbol{\epsilon}^T(I_N-A (A^T A)^{-1} A^T)\boldsymbol{\epsilon}]= {\rm tr}(I)-{\rm tr}(A (A^T A)^{-1} A^T)=N-p\] <p>For non-linear models, we linearize it with the optimal solution \(\boldsymbol{\hat{\theta}}\) which should lay near the truth \(\boldsymbol{\theta}_0\) and use the arguments we presented just above to yield</p> \[\mathbb{E}[\chi^2]\approx N-p.\] </details> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:3" role="doc-endnote"> <p>at least I certainly haven’t until I wrote this <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>which I have seen in many places incorrectly stated or incorrectly attributed <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4" role="doc-endnote"> <p>The paper is a fantastic read and I would highly recommend checking it out when you can! <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="cool-math-stuff"/><category term="DataFitting"/><category term="Optimization"/><category term="Probability"/><summary type="html"><![CDATA[An introduction to fitting data and understanding the chi-squared statistic]]></summary></entry><entry><title type="html">Least-squares and fitting curves through points</title><link href="https://dvdjsp.github.io/blog/2026/LeastSq/" rel="alternate" type="text/html" title="Least-squares and fitting curves through points"/><published>2026-01-15T00:00:00+00:00</published><updated>2026-01-15T00:00:00+00:00</updated><id>https://dvdjsp.github.io/blog/2026/LeastSq</id><content type="html" xml:base="https://dvdjsp.github.io/blog/2026/LeastSq/"><![CDATA[<style>
.question-box {
  border: 2px solid #a463bf;
  border-radius: 8px;
  padding: 20px;
  margin-bottom: 25px;
  background-color: #f9f9f9;
}

.question-box h3 {
  color: #a463bf;
  margin-top: 0;
  font-size: 1.3em;
  font-weight: 700;
}

.question-content {
  font-size: 1.05em;
  line-height: 1.6;
  color: #333;
  margin-bottom: 15px;
}

details {
  border: 1px solid #ddd;
  border-radius: 6px;
  margin-top: 15px;
  background-color: #fff;
}

details > summary {
  cursor: pointer;
  padding: 12px 15px;
  background-color: #a463bf;
  color: white;
  font-weight: 600;
  font-size: 1.05em;
  border-radius: 5px;
  transition: background-color 0.2s;
}

details > summary:hover {
  background-color: #8b4ba3;
}

details[open] > summary {
  border-radius: 5px 5px 0 0;
  margin-bottom: 15px;
}

.solution-content {
  padding: 15px;
  line-height: 1.7;
  color: #333;
}

.solution-content p {
  margin-bottom: 10px;
}

.page-header {
  text-align: center;
  margin-bottom: 40px;
  padding-bottom: 20px;
  border-bottom: 3px solid #a463bf;
}

.page-header h1 {
  color: #a463bf;
  margin-bottom: 10px;
}

.page-header p {
  color: #666;
  font-size: 1.1em;
}

/* Dark mode styles */
html[data-theme='dark'] .question-box {
  background-color: #1a1a1a;
  border-color: #2698ba;
}

html[data-theme='dark'] .question-box h3 {
  color: #2698ba;
}

html[data-theme='dark'] .question-content {
  color: #e0e0e0;
}

html[data-theme='dark'] details {
  background-color: #2a2a2a;
  border-color: #444;
}

html[data-theme='dark'] details > summary {
  background-color: #2698ba;
}

html[data-theme='dark'] details > summary:hover {
  background-color: #3bb5d9;
}

html[data-theme='dark'] .solution-content {
  color: #e0e0e0;
}

html[data-theme='dark'] .page-header h1 {
  color: #2698ba;
}

html[data-theme='dark'] .page-header p {
  color: #b0b0b0;
}

/* Remark box styles */
.remark-box {
  border: 2px solid #e57373;
  border-radius: 8px;
  padding: 20px;
  margin-bottom: 25px;
  background-color: #ffebee;
}

.remark-box h3 {
  color: #d32f2f;
  margin-top: 0;
  font-size: 1.3em;
  font-weight: 700;
}

.remark-content {
  font-size: 1.05em;
  line-height: 1.6;
  color: #333;
}

/* Dark mode styles for remark box */
html[data-theme='dark'] .remark-box {
  background-color: #1a1a1a;
  border-color: #4db6ac;
  background-color: #0d2a28;
}

html[data-theme='dark'] .remark-box h3 {
  color: #4db6ac;
}

html[data-theme='dark'] .remark-content {
  color: #e0e0e0;
}
</style> <p><em>Given a bunch of data, how can we find a <strong>representative</strong> curve that approximates the data?</em></p> <p>Let us understand this question through an example. Suppose that you’re out there trying to measure the resistance of a resistor by studying its Voltage vs Current using the relation \(V=I R\). Unfortunately, your equipment is not up to the standard you’d expect: it produces noisy data and you observe the following</p> <pre><code class="language-plotly">{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 11,
      "mode": "markers",
      "name": "Measured Data",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Voltage vs Current (Noisy Measurements)"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
</code></pre> <p><em>If only</em> this was a nice straight line, then using \(V=I R\), you can immediately extract the resistance by finding its slope. However, the real world is messy and chaotic. So instead of wishing for what could have been, lets try to work with what we have: <em>what if we could extract a straight line from the data?</em> A “line of best fit” that passes through the points <strong>as close as possible</strong>. If we have that line, then the resistance is <em>approximately</em> the slope of that line. So a natural question to ask is <strong>how can one find this line?</strong></p> <p>Let’s denote this arbitrary line, i.e our <strong>model</strong> as a function \(f(x;m,c)\). A <strong>model</strong> is a function of your input variables (your control variables) and some parameters you want to estimate. Usually, a model is motivated by the physics and some underlying principles, but importantly it is something that you have to <em>declare</em>. In our case, the input variable is denoted by \(x\) (which is current here) and our parameters are the familiar <strong>parameters</strong> \(m\) (slope) and \(c\) (intercept) defining \(f(x; m,c)\) through</p> \[f(x;m,c)= m x +c.\] <p>The two parameters \((m,c)\) uniquely determines a line and therefore our special model is called a linear model. In principle, there are infinitely many values of \((m,c)\) and it is our goal to comb through this forest of \(m\) and \(c\) to find the correct \(m_0\) and \(c_0\) for which the line \(f(x;m_0,c_0)\) passes the “closest” to our data points. On that note, <em>What does “closest” even mean</em>?</p> <div class="question-box"> <h3 id="what-does-closest-mean">What does closest mean?</h3> <p>Suppose you’ve collected \(N\) data points \(y_i\) for every input (control variable) value \(x_i\) where \(i=1,2,\dots,N\), then for every data point we can find \(c\) and \(m\) that minimizes the distance between the data point and your model that is given by \(\vert y_i - f(x_i; m,c)\vert\). This distance has a name, it is called the <strong>residue</strong>. We can mathematically write our process of minimizing the distance between our model and our data as over all the possible values of \(m\) and \(c\) as</p> \[\min_{m, c} \{\vert y_i-f(x_i;m ,c)\vert\} \text{ for all } i=1,2,\dots N.\] </div> <p>This is fine in theory but not in practice as you’d have to sample all the infinitely many values of \(m\) and \(c\) which can take forever. However, you remember from Calculus 1 that an optimization problem can be written in terms of a derivative (set to \(0\)), so you might wonder if there is a way to implement this in our problem.</p> <div class="question-box"> <h3 id="minimization-using-calculus">Minimization using Calculus</h3> <p>It turns out, you <strong>can</strong> rewrite this in terms of a function whose derivative set to \(0\) gives us the optimal solution, but we need to make a few compromises:</p> <ol> <li>Change \(\vert X \vert\) to \(X^2\), since the absolute value function is notoriously non-differentiable at \(X=0\) and</li> <li>Minimize the sum of the \(\text{distance}^2\) of all the data points instead of minimizing the distance individually.</li> </ol> <p>(1) and (2) together is equivalent to minimizing the following <strong>objective function</strong> \(\mathcal{L}\) over all possible values of \(m\) and \(c\)</p> \[\mathcal{L}(m,c)=\sum_{i=1}^{N} [y_i- f(x_i; m,c)]^2.\] <p>The above formalism is called the <a href="https://en.wikipedia.org/wiki/Least_squares">least-squares method</a>, since you want to minimize the (sum) squared distance of the measured data points and the model.</p> </div> <p>In our case, minimizing the least squares (our objective function here) is the same as finding solutions \((m,c)\) where the derivatives vanish.</p> <div class="question-box"> <h3 id="what-kind-of-derivatives">What kind of derivatives?</h3> <p>If our model only had one parameter \(\theta\), then it would be an ordinary derivative \(\frac{d}{d\theta}\). However, since we’ve got two parameters, we need to consider the <strong>partial</strong> derivatives in both, i.e we solve</p> \[\begin{align} \frac{\partial}{\partial m} \mathcal{L}(m,c)&amp;=0\\ \frac{\partial}{\partial c} \mathcal{L}(m,c)&amp;=0 \end{align}\] <p>Solving for both simultaneously, we’ll be able to find the optimum solution \(m_0\) and \(c_0\) for which our line \(f(x;m_0,c_0)=m_0 x +c_0\) is the closet to our observed data. In the special case of a linear model , we can find the unique solution exactly (try working it out!).</p> </div> <p>Usually these calculations are done at the blink of an eye through existing packages like <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html"><code class="language-plaintext highlighter-rouge">curve_fit()</code></a> from <code class="language-plaintext highlighter-rouge">scipy.optimize</code> in <code class="language-plaintext highlighter-rouge">python</code>. Here’s what the least squares fit looks like for our noisy data:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 15,
      "mode": "markers",
      "name": "Measured Data",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    },
    {
      "fitFromTrace": 0,
      "points": 100,
      "mode": "lines",
      "name": "Least Squares Fit",
      "line": {
        "color": "#000101ff",
        "width": 3,
        "dash": "solid"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Voltage vs Current with Least Squares Fit"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
</code></pre> <div class="remark-box"> <h3 id="remark">Remark</h3> <div class="remark-content"> An important takeaway from least squares fitting is that the entire problem is **merely** an optimization problem: for a fixed given set of data, find the curve that is the closest to it, i.e minimizes the least square error </div> </div> <div class="question-box"> <h3 id="what-lies-beyond-the-linear-model">What lies beyond the linear model?</h3> <p>After dabbling with the simple linear model, we may wonder what lies beyond it: What if we’d like to fit a parabola through your data points? What if an inverse square? For cases when linearization is possible, <strong>always linearize</strong>, i.e if your model was \(f(x; a,b)= a x^2 +b\), then set \(X=x^2\) and then you have a linear model \(g(X; a,b)= a X +b\). However, in cases where such a linearization is not possible and your model is something like \(a x^2 +b x +c\) then you continue systematically and solve the set of equations</p> \[\begin{align} \frac{\partial}{\partial a} \mathcal{L}(a,b,c)&amp;=0\\ \frac{\partial}{\partial b} \mathcal{L}(a,b,c)&amp;=0\\ \frac{\partial}{\partial c} \mathcal{L}(a,b,c)&amp;=0 \end{align}\] <details style="padding: 15px 20px;"> <summary>Model with multiple parameters</summary> <p>If you had a general model with \(p\) parameters \(\theta_1,\theta_2,\dots,\theta_M\) that we can gather as an array or vector commonly denoted with a bold \(\boldsymbol{\theta}\)</p> \[\boldsymbol{\theta}=(\theta_1,\theta_2,\dots \theta_M),\] <p>for a model \(f(x; \boldsymbol{\theta})\),</p> <p>Then we solve the set of equations</p> \[\begin{align} \frac{\partial}{\partial \theta_1} \mathcal{L}(\boldsymbol{\theta})&amp;=0\\ \frac{\partial}{\partial \theta_2} \mathcal{L}(\boldsymbol{\theta})&amp;=0\\ \vdots &amp; =0 \\ \frac{\partial}{\partial \theta_M} \mathcal{L}(\boldsymbol{\theta})&amp;=0\\ \end{align}\] <p>for the optimum solution \(\boldsymbol{\hat{\theta}}\) which minimizes our objective function defined by</p> \[\mathcal{L}(\boldsymbol{\theta})= \sum_{i=1}^{N} [y_i - f(x_i; \boldsymbol{\theta})]^2.\] <p>Our optimal solution \(\boldsymbol{\hat{\theta}}\) brings our model \(f(x; \boldsymbol{\hat{\theta}})\) the closest to our data points.</p> </details> </div> <p>Now you’ve figured out how to fit data and you happily move on with your life, succeeding in extracting the resistance until your friend comes along and performs the same experiment and observes the following</p> <pre><code class="language-plotly">{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 11,
      "mode": "markers",
      "name": "Measured Data",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Voltage vs Current (Friend's)"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
</code></pre> <p>You notice that their data is <strong>not</strong> exactly the same as yours (look above!). As special as your friend is to you, their data isn’t. Indeed, anyone perform the same set of measurement (called a trial) and get different data \(y_i\). At this stage, you realize that your data set is one out of the infinity many <em>possible</em> realizations, so it is natural to somehow factor that <em>uncertainty</em> in your measurement.</p> <p>How do we do that you say? Through something called the \(\chi^2\) test. <a href="/blog/2026/Chisq/">What the heck is \(\chi^2\) anyway?</a></p>]]></content><author><name></name></author><category term="cool-math-stuff"/><category term="DataFitting"/><category term="Optimization"/><summary type="html"><![CDATA[An introduction to the least squares method]]></summary></entry><entry><title type="html">Guess The Date!</title><link href="https://dvdjsp.github.io/blog/2024/Shannon/" rel="alternate" type="text/html" title="Guess The Date!"/><published>2024-11-29T00:00:00+00:00</published><updated>2024-11-29T00:00:00+00:00</updated><id>https://dvdjsp.github.io/blog/2024/Shannon</id><content type="html" xml:base="https://dvdjsp.github.io/blog/2024/Shannon/"><![CDATA[<p>Soon to come!</p>]]></content><author><name></name></author><category term="cool-math-stuff"/><category term="InformationTheory"/><category term="ShannonEntropy"/><category term="Probability"/><summary type="html"><![CDATA[An introduction to Shannon Entropy and quantifying information.]]></summary></entry></feed>