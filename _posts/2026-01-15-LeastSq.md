---
layout: post
title: "Least-squares and fitting curves through points"
date: 2026-01-15
description: An introduction to the least squares method
tags: DataFitting Optimization Probability 
categories: cool-math-stuff
tabs: true
chart:
  plotly: true
---

<style>
.question-box {
  border: 2px solid #a463bf;
  border-radius: 8px;
  padding: 20px;
  margin-bottom: 25px;
  background-color: #f9f9f9;
}

.question-box h3 {
  color: #a463bf;
  margin-top: 0;
  font-size: 1.3em;
  font-weight: 700;
}

.question-content {
  font-size: 1.05em;
  line-height: 1.6;
  color: #333;
  margin-bottom: 15px;
}

details {
  border: 1px solid #ddd;
  border-radius: 6px;
  margin-top: 15px;
  background-color: #fff;
}

details > summary {
  cursor: pointer;
  padding: 12px 15px;
  background-color: #a463bf;
  color: white;
  font-weight: 600;
  font-size: 1.05em;
  border-radius: 5px;
  transition: background-color 0.2s;
}

details > summary:hover {
  background-color: #8b4ba3;
}

details[open] > summary {
  border-radius: 5px 5px 0 0;
  margin-bottom: 15px;
}

.solution-content {
  padding: 15px;
  line-height: 1.7;
  color: #333;
}

.solution-content p {
  margin-bottom: 10px;
}

.page-header {
  text-align: center;
  margin-bottom: 40px;
  padding-bottom: 20px;
  border-bottom: 3px solid #a463bf;
}

.page-header h1 {
  color: #a463bf;
  margin-bottom: 10px;
}

.page-header p {
  color: #666;
  font-size: 1.1em;
}

/* Dark mode styles */
html[data-theme='dark'] .question-box {
  background-color: #1a1a1a;
  border-color: #2698ba;
}

html[data-theme='dark'] .question-box h3 {
  color: #2698ba;
}

html[data-theme='dark'] .question-content {
  color: #e0e0e0;
}

html[data-theme='dark'] details {
  background-color: #2a2a2a;
  border-color: #444;
}

html[data-theme='dark'] details > summary {
  background-color: #2698ba;
}

html[data-theme='dark'] details > summary:hover {
  background-color: #3bb5d9;
}

html[data-theme='dark'] .solution-content {
  color: #e0e0e0;
}

html[data-theme='dark'] .page-header h1 {
  color: #2698ba;
}

html[data-theme='dark'] .page-header p {
  color: #b0b0b0;
}
</style>

*Given a bunch of data, how can we find a **representative** curve that approximates the data?*

Let us understand this question through an example. Suppose that you're out there trying to measure the resistance of a resistor by studying its Voltage vs Current using the relation $$V=I R$$. Unfortunately, your equipment is not up to the standard you'd expect: it produces noisy data and you observe the following


```plotly
{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 11,
      "mode": "markers",
      "name": "Measured Data",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Voltage vs Current (Noisy Measurements)"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
```
*If only* this was a nice straight line, then using $$V=I R$$, you can immediately extract the resistance by finding its slope. However, the real world is messy and chaotic. So instead of wishing for what could have been, lets try to work with what we have: *what if we could extract a straight line from the data?* A "line of best fit" that passes through the points **as close as possible**. If we have that line, then the resistance is *approximately* the slope of that line. So a natural question to ask is **how can one find this line?**

Let's denote this arbitrary line, i.e our **model** as a function $$f(x;m,c)$$. A **model** is a function of your input variables (your control variables) and some parameters you want to estimate. Usually, a model is motivated by the physics and some underlying principles, but importantly it is something that you have to *declare*. In our case, the input variable is denoted by $$x$$ (which is current here) and our parameters are the familiar **parameters** $$m$$ (slope) and $$c$$ (intercept) defining $$f(x; m,c)$$ through


$$ f(x;m,c)= m x +c.$$

The two parameters $$(m,c)$$ uniquely determines a line and therefore our special model is called a linear model. In principle, there are infinitely many values of $$(m,c)$$ and it is our goal to comb through this forest of $$m$$ and $$c$$ to find the correct $$m_0$$ and $$c_0$$ for which the line $$f(x;m_0,c_0)$$ passes the "closest" to our data points. On that note, *What does "closest" even mean*?

<div class="question-box" markdown="1">

### What does closest mean?

Suppose you've collected $$N$$ data points $$y_i$$ for every input (control variable) value $$x_i$$ where $$ i=1,2,\dots,N$$, then for every data point we can find $$c$$ and $$m$$ that minimizes the distance between the data point and your model that is given by $$\vert y_i - f(x_i; m,c)\vert$$. This distance has a name, it is called the **residue**. We can mathematically write our process of minimizing the distance between our model and our data as over all the possible values of $$m$$ and $$c$$ as

$$\min_{m, c} \{\vert y_i-f(x_i;m ,c)\vert\} \text{ for all } i=1,2,\dots N.$$

</div>

This is fine in theory but not in practice as you'd have to sample all the infinitely many values of $$m$$ and $$c$$ which can take forever. However, you remember from Calculus 1 that an optimization problem can be written in terms of a derivative (set to $$0$$), so you might wonder if there is a way to implement this in our problem. 

<div class="question-box" markdown="1">

### Minimization using Calculus

It turns out, you **can** rewrite this in terms of a function whose derivative set to $$0$$ gives us the optimal solution, but we need to make a few compromises: 

1. Change $$\vert X \vert$$ to $$X^2$$, since the absolute value function is notoriously non-differentiable at $$X=0$$ and 
2. Minimize the sum of the $$\text{distance}^2$$ of all the data points instead of minimizing the distance individually.

(1) and (2) together is equivalent to minimizing the following **objective function** $$\mathcal{L}$$ over all possible values of $$m$$ and $$c$$ 

$$ \mathcal{L}(m,c)=\sum_{i=1}^{N} [y_i- f(x_i; m,c)]^2.$$

The above formalism is called the [least-squares method](https://en.wikipedia.org/wiki/Least_squares), since you want to minimize the (sum) squared distance of the measured data points and the model.
</div>

In our case, minimizing the least squares (our objective function here) is the same as finding solutions $$(m,c)$$ where the derivatives vanish. 

<div class="question-box" markdown="1">

### What kind of derivatives? 

If our model only had one parameter $$\theta$$, then it would be an ordinary derivative $$\frac{d}{d\theta}$$. However, since we've got two parameters, we need to consider the **partial** derivatives in both, i.e we solve

$$ \begin{align} 
\frac{\partial}{\partial m} \mathcal{L}(m,c)&=0\\
\frac{\partial}{\partial c} \mathcal{L}(m,c)&=0
\end{align}$$

Solving for both simultaneously, we'll be able to find the optimum solution $$m_0$$ and $$c_0$$ for which our line $$f(x;m_0,c_0)=m_0 x +c_0$$ is the closet to our observed data. In the special case of a linear model , we can find the unique solution exactly (try working it out!). 
</div>

Usually these calculations are done at the blink of an eye through existing packages like [```curve_fit()```](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html) from ```scipy.optimize``` in ```python```. Here's what the least squares fit looks like for our noisy data:

```plotly
{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 15,
      "mode": "markers",
      "name": "Measured Data",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    },
    {
      "fitFromTrace": 0,
      "points": 100,
      "mode": "lines",
      "name": "Least Squares Fit",
      "line": {
        "color": "#000101ff",
        "width": 3,
        "dash": "solid"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Voltage vs Current with Least Squares Fit"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
```

<div class="question-box" markdown="1">

### What lies beyond the linear model?
After dabbling with the simple linear model, we may wonder what lies beyond it: What if we'd like to fit a parabola through your data points? What if an inverse square? For cases when linearization is possible, **always linearize**, i.e if your model was $$f(x; a,b)= a x^2 +b$$, then set $$X=x^2$$ and then you have a linear model $$g(X; a,b)= a X +b$$. However, in cases where such a linearization is not possible and your model is something like $$a x^2 +b x +c$$ then you continue systematically but solve the set of equations 

$$ \begin{align} 
\frac{\partial}{\partial a} \mathcal{L}(a,b,c)&=0\\
\frac{\partial}{\partial b} \mathcal{L}(a,b,c)&=0\\
\frac{\partial}{\partial c} \mathcal{L}(a,b,c)&=0
\end{align}$$

<details markdown="1" style="padding: 15px 20px;">
<summary>Model with multiple parameters and multiple inputs</summary>

If you had a general model with $$M$$ parameters $$\theta_1,\theta_2,\dots,\theta_M$$ that takes in $$P$$ inputs $$\boldsymbol{x}=(x_1,x_2,\dots, x_P)$$ given by $$f(\boldsymbol{x}; \boldsymbol{\theta})$$ that we can gather as an array or vector commonly denoted with a bold $$\boldsymbol{\theta}$$

$$ \boldsymbol{\theta}=(\theta_1,\theta_2,\dots \theta_M),$$

Then we solve the set of equations 

$$ \begin{align} 
\frac{\partial}{\partial \theta_1} \mathcal{L}(\boldsymbol{\theta})&=0\\
\frac{\partial}{\partial \theta_2} \mathcal{L}(\boldsymbol{\theta})&=0\\
\vdots & =0 \\
\frac{\partial}{\partial \theta_M} \mathcal{L}(\boldsymbol{\theta})&=0\\
\end{align}$$

for the optimum solution $$\boldsymbol{\theta}_0$$ which minimizes our objective function defined by 

$$\mathcal{L}(\boldsymbol{\theta})= \sum_{i=1}^{N} [y_i - f(\boldsymbol{x}_i; \boldsymbol{\theta})]^2.$$

Our optimal solution $$\boldsymbol{\theta}_0$$ brings our model $$f(\boldsymbol{x}; \boldsymbol{\theta}_0)$$ the closest to our data points.
</details>
</div>

Now you've figured out how to fit data and you happily move on with your life, succeeding in extracting the resistance until your friend comes along and performs the same experiment and observes the following 


```plotly
{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 11,
      "mode": "markers",
      "name": "Measured Data",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Voltage vs Current (Friend's)"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
```
You notice that their data is **not** exactly the same as yours (look above!). As special as your friend is to you, their data isn't. Indeed, anyone perform the same set of measurement (called a trial) and get different data $$y_i$$. At this stage, you realize that your data set is one out of the infinity many *possible* realizations, so it is natural to somehow factor that *uncertainty* in your measurement. 

How do we do that you say? Through the $$\chi^2$$ test that you can read about here: [What the heck is $$\chi^2$$ anyway?]({% post_url 2026-01-16-Chisq %}) 
