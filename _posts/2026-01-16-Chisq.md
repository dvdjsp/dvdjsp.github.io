---
layout: post
title: "What the heck is \\(\\chi^2\\) anyway?"
date: 2026-01-16
description: An introduction to fitting data and understanding the chi-squared statistic
tags: DataFitting Optimization Probability 
categories: cool-math-stuff
tabs: true
chart:
  plotly: true
---


*How do you fit a curve through data? What's a statistic? and most importantly **what is being squared?*** 

These are important questions that you might encounter when you look up curve fitting! This is a continuation on *how to fit data* where I answer the former in [Least-squares and fitting curves through points]({% post_url 2026-01-15-LeastSq %}). Before getting into what a statistic is, let's first begin by answering the most common and important question here: 

<style>
.question-box {
  border: 2px solid #a463bf;
  border-radius: 8px;
  padding: 20px;
  margin-bottom: 25px;
  background-color: #f9f9f9;
}

.question-box h3 {
  color: #a463bf;
  margin-top: 0;
  font-size: 1.3em;
  font-weight: 700;
}

.question-content {
  font-size: 1.05em;
  line-height: 1.6;
  color: #333;
  margin-bottom: 15px;
}

details {
  border: 1px solid #ddd;
  border-radius: 6px;
  margin-top: 15px;
  background-color: #fff;
}

details > summary {
  cursor: pointer;
  padding: 12px 15px;
  background-color: #a463bf;
  color: white;
  font-weight: 600;
  font-size: 1.05em;
  border-radius: 5px;
  transition: background-color 0.2s;
}

details > summary:hover {
  background-color: #8b4ba3;
}

details[open] > summary {
  border-radius: 5px 5px 0 0;
  margin-bottom: 15px;
}

.solution-content {
  padding: 15px;
  line-height: 1.7;
  color: #333;
}

.solution-content p {
  margin-bottom: 10px;
}

.page-header {
  text-align: center;
  margin-bottom: 40px;
  padding-bottom: 20px;
  border-bottom: 3px solid #a463bf;
}

.page-header h1 {
  color: #a463bf;
  margin-bottom: 10px;
}

.page-header p {
  color: #666;
  font-size: 1.1em;
}

/* Dark mode styles */
html[data-theme='dark'] .question-box {
  background-color: #1a1a1a;
  border-color: #2698ba;
}

html[data-theme='dark'] .question-box h3 {
  color: #2698ba;
}

html[data-theme='dark'] .question-content {
  color: #e0e0e0;
}

html[data-theme='dark'] details {
  background-color: #2a2a2a;
  border-color: #444;
}

html[data-theme='dark'] details > summary {
  background-color: #2698ba;
}

html[data-theme='dark'] details > summary:hover {
  background-color: #3bb5d9;
}

html[data-theme='dark'] .solution-content {
  color: #e0e0e0;
}

html[data-theme='dark'] .page-header h1 {
  color: #2698ba;
}

html[data-theme='dark'] .page-header p {
  color: #b0b0b0;
}
</style>

<div class="question-box" markdown="1">

### What is being squared?

**Nothing!** This is a trick question used by cruel people. Unfortunately, is a great example of an abuse of notation: $$\chi^2$$ is a variable, just like $$x$$ or $$y$$. We could have called it $$\chi$$, but for historical reasons, [Pearson called it $$\chi^2$$.](https://www.tandfonline.com/doi/abs/10.1080/14786440009463897)

</div>


Now that we've tackled this important question, let us motivate the statistic with the same example we used previously: 

You (and your friend) are trying to measure the resistance of a resistor by studying its Voltage vs Current using the relation $$V=I R$$. Unfortunately, your equipment isn't the greatest and you get noisy data that is "roughly" linear. Adding to that, neither of your sets of measurements are identical, i.e not only are they not on a line, they are different for the both of you! 

```plotly
{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 15,
      "mode": "markers",
      "name": "Trial 1",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "First Measurement"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
```

```plotly
{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 15,
      "mode": "markers",
      "name": "Trial 2",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Second Measurement"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
```

It seems that there isn't *just* an uncertainty that prevents the data from landing perfectly on a line, but also in addition, each data point **has uncertainties** 

Earlier we laid out the ideas behind curve fitting, but now we can now answer the more physical question of fitting data **with uncertainties** which is where $$\chi^2$$ comes into play. The $$\chi^2$$ statistic (pronounced **chi-squared**) is a metric used to asses a fit to a collection of data. To work with $$\chi^2$$, we first need some **important** ideas from statistics so we'll start here. If you want to skip to using it, then click on [how to use $$\chi^2$$ goodness of fit](#how-chi2-goodness-of-fit-is-used).

# <span style="color: #a463bf;">Some statistics overview</span> 

<div class="question-box" markdown="1">

### Dr. Strange and the very many different outcomes

You put on your thinking cap and recognize that each of the data point you measure is really one possible outcome that you experience. Like Dr. Strange using his ability to view all the possible multiverses, you view each data point you see as a specific outcome, and the collection of all possible outcomes together is what you would get if you repeat the experiment infinitely many times. In this sense, we say that the the data points are **sampled from a distribution** which controls the probability of it showing up as a special number. Mathematically, what this means is that all the data points $$y_i$$ are **random variables**.

What this implies is that the more data you measure, the more information you get about each data point and therefore the more accurate your results become. The true values of each data point should be the mean of all the data you have for that particular data point. There The variation is its standard error of the mean and you plot them together with error bars.
</div> 

To get an intuitive feel for what this means, lets think about it in terms of an excel sheet. If you imagine an excel sheet keeping track of your measured data with each row corresponding to its trial number and each column being the measured values for the $$i^{\rm th }$$ data point $$y_i$$, then isolating a column (say the first column of $$y_1$$) over **all trials** gives you the different possible values that data point can $$y_i$$ take. How do we use this information? We **average!**

The idea is to collapse the information of multiple plots by aggregating a specific column and plotting the corresponding averages as data points $$\bar{y}_i$$

```plotly
{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 15,
      "mode": "markers",
      "name": "Data ",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      },
      "error_y": {
        "type": "constant",
        "value": 1.0,
        "visible": true,
        "color": "#a463bf",
        "thickness": 2,
        "width": 4
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Voltage vs Current with Error Bars"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
```
where we **call** the error bars of size $$\sigma_{\bar{y}_i}$$ as the standard error of the mean for each column.
<details markdown="1" style="padding: 15px 20px;">
<summary>Population and Sample mean, error and standard error </summary>
In statistics, a lot of similar sounding terms can be thrown around that can lead to confusion. Here I would like to clarify the difference between **population** and **sample** data set. The quantities "populaton mean", "sample standard deviation" etc are known as a measured **statistic**. 

In a nutshell, the population data set is the true data set, everyone and everything that you seek to study is present in the data set. Its mean (called the **population mean** denoted by $$\mu$$) and standard deviation (called the **population standard deviation** denoted by $$\sigma$$) is the **true** or correct statistic. However, in real life, it's often not possible to get the whole collection, so we work with a small subset called the **sample data set**, i.e **your** data set.

* The sample mean $$\bar{s}$$ corresponds to the mean of your column, it is what we call an **estimator** of the true population mean

* The sample standard deviation $$s$$ is the standard deviation of your column computed using [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction), it is what we call an **estimator** of the true population standard deviation.

So far, all of this is purely based on the data you collect, and nothing to do with the population mean or variance directly, only that they approximate or estimate the true values. Now comes the sample standard error (or standard error for short) that acts as a bridge between your data and the whole data set, 

* The sample standard error is sort of a bridge between the population and sample data set: it tells you roughly how far your sample mean is compared to the actual, true population mean and is defined by 

  $$ \sigma_{\bar{s}}= \frac{s}{\sqrt{N}}$$

</details>

Now that we know of a method to encode the information that we get form multiple trials through aggregating, we notice that for different data sizes, we get different means and so the means themselves are a random variable. So we ask:


<div class="question-box" markdown="1">
### What distribution does the data point $$\bar{y}_i$$ come from?

This question is a genuinely hard question that is not possible to answer exactly. The simplest answer is to make an educated guess. However, given certain conditions something truly spectacular happens: you can assume that the data point $$y_i$$  comes from a **Normal distribution** $$\mathcal{N}(\mu_i,\sigma_i)$$ through something called the **Central Limit Theorem**. 

Before we get into this, let's think about the weight of this conclusion. Out of the many distributions you know like the humble [uniform distribution](), or the [Poisson](https://en.wikipedia.org/wiki/Poisson_distribution) or the [Lorentzian](https://en.wikipedia.org/wiki/Cauchy_distribution) or the many many more distributions you don't know (I certainly didn't until now) like the [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) or the [Continuous Bernoulli Distribution](https://en.wikipedia.org/wiki/Continuous_Bernoulli_distribution), we arrive at the seemingly innocent Normal distribution!

This Normal distribution or the "bell shaped curve" with mean $$\mu$$ and **variance** $$\sigma^2$$ is given by the following equation

$$\mathcal{N}(\mu,\sigma^2)= \frac{1}{\sqrt{2\sigma^2 \pi}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}$$

and is the renowned "Gaussian distribution" but normalized (i.e the area under this curve is $$=1$$). 

The Central Limit Theorem implies that the sample mean we were measuring $$\bar{y}_i$$ using $$N$$ trials comes from a normal distribution represented mathematically as 

$$ \bar{y}_i \sim \mathcal{N}\left(\mu,\frac{\sigma^2}{N}\right),$$

where $$\mu_i$$ is the true population mean and $$\sigma_i$$, the population standard deviation, both of which are unknown. However, just because they are unknown does not mean that we can't approximate them. In fact the sample standard error of the mean (from your data set) is an estimator and can be used instead


$$\frac{\sigma^2}{N}=\sigma_{\mu}\approx \sigma_{\bar{s}}$$

<details markdown="1" style="padding: 15px 20px;">
<summary>Central Limit Theorem: Why <u>the</u> Normal distribution?</summary>
The [Central Limit Theorem (CLT)](https://en.wikipedia.org/wiki/Central_limit_theorem) is what we used to justify the emergence of the Normal distribution. This theorem is colloquially stated along the lines of **"more data means more bell shaped"** but this isn't quite the correct statement of the theorem and can lead to a lot of misunderstandings. There are a few conditions for the CLT to hold true, and they are nicely discussed in [3B1B's "But what is the Central Limit Theorem?"](https://www.youtube.com/watch?v=zeJD6dqJ5lo) that I will paraphrase below:

1. **Independence**: The data you collect should be independent of each other, i,e measuring one does not affect the next measurement

2. **Same distribution**: The data you collect, i.e the data points in the columns in your excel sheet should come from the same distribution 

3. **Finite variance**: Your error bars must be finite, i.e more data does not increase the variance you observe for a single column.

Denote by $$y_i^{(j)}$$ the data points in the $$i^{\rm th}$$ column and $$j^{\rm th}$$ row. Given the above conditions, CLT states that in the limit as you perform many many many trials (i.e the number of rows $$N$$ goes to infinity),

The quantity 

$$ \lim_{N\to \infty} \frac{\sum_{j=1}^N y_i^{(j)}}{N}$$

which is just the average of our column follows

$$ \lim_{N\to \infty} \frac{\frac{\sum_{j=1}^N y_i^{(j)}}{N}- \mu_i}{\frac{\sigma_i}{\sqrt{N}}}\sim \mathcal{N}(0,1)$$

follows a Normal distribution with mean zero and standard deviation one!

 This means the average in a fixed column in the limit of more data points follows a normal distribution around the population mean $$\mu_i$$ and standard deviation which is the standard error of the mean:

$$\lim_{N\to \infty} \frac{\sum_{j=1}^N y_i^{(j)}}{N}\sim \mathcal{N}\left(\mu_i,\frac{\sigma_i}{\sqrt{N}}\right)$$

 </details>
</div>

All that jazz to say that for our fitting purposes, we know the kind of errors we **need** to account for in our real experiments.

# <span style="color: #a463bf;" id="how-chi2-goodness-of-fit-is-used">How $$\chi^2$$ goodness of fit is used</span> 

But *how* do we account for this errors? If we **didn't** have errors per data point, then we'd stick to our usual least-squares method and minimize the least squares. Essentially, we're trying to make the distance between our model $$f(\boldsymbol{x},\boldsymbol{\theta})$$ and our data points $$y_i$$ **small**, but with errors, *what does small mean?* i.e, **small with respect to what?**

In the case where data points have errors, a natural "scale" we're trying to compare this is against the fluctuations--the error $$\sigma_i$$ per data point. The error is estimated using multiple trials for a data point or guesstimated based on the order of magnitude. With this scale in mind, we minimize this sort of modified or scaled least-squares which is what we call $$\chi^2$$

$$\begin{equation}\sum_{i=1}^N \left(\frac{y_i- f(\boldsymbol{x};\boldsymbol{\theta})}{\sigma_i}\right)^2:= \chi^2.\end{equation}$$

If we're fitting a **line** to our data points with errors (as in the example of $$V$$ vs $$I$$), then we'd use $$f(\boldsymbol{x};\boldsymbol{\theta})=f(x;m,c):=m x +c$$ for the free parameters of the line $$m$$ (slope) and $$c$$ (intercept). Since errors $$\sigma_i$$ are just (estimated or guesstimated) and comes in as a multiplicative factor to residues, the method to find the optimal solution is identical to what we did in [Least-squares and fitting curves through points]({% post_url 2026-01-15-LeastSq %}). Since $$\chi^2$$ is just least squares accounting for the error for each data point, in ```python```, we'd still use [```scipy.optimize.curve_fit()```](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html) with ```sigma``` (make sure to set ```absolute_sigma=True```). 

**Note:** ```scipy.optimize.curve_fit()``` does not return $$\chi^2$$, it returns the parameters of your model i.e your fit. You'd have to compute $$\chi^2$$ by using those parameters in Eq. (1)

<div class="question-box" markdown="1">

### What value of $$\chi^2$$ corresponds to a "good" fit?
$$\chi^2$$ by itself for a particular fit does not tell us much: *Should it be very small? Should it be very big?* A common (but understandable) misconception is that since $$\chi^2$$ comes from least-squares, obtaining the smallest value of $$\chi^2$$ should correspond to a good fit since that worked for least squares. However, this is **not** true! The quantity you need to check against is the so called **reduced chi-squared** defined by

$$ \chi^2_{\nu} := \frac{\chi^2}{\nu}$$

where $$\nu=N-p$$ is called the **degree of freedom** that you can find by subtracting the number of data points with the number of parameters you used for the model. For example: If you had 10 data points that you're trying to fit a line to, then $$\nu=10-2=8$$. 

**What value of $$\chi^2_{\nu}$$ is good?** It turns out that the expected value (average) of $$\chi^2_{\nu}$$ for a good fit is one, i.e

$$ \mathbb{E}[\chi^2_{\nu}]= 1$$

meaning that a fit is "good" if 

$$\chi^2_{\nu} \approx 1$$ 

<details markdown="1" style="padding: 15px 20px;">
<summary>Why is \(\chi^2_{\nu}\) expected to be one?</summary>

</details>
</div>

Now that we've equipped you with the basics of curve fitting, I wish you the best on your fitting adventures!