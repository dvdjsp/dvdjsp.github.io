---
layout: post
title: "What the heck is \\(\\chi^2\\) anyway?"
date: 2026-01-15
description: An introduction to fitting data and understanding the chi-squared statistic
tags: DataFitting Optimization Probability 
categories: cool-math-stuff
tabs: true
chart:
  plotly: true
---


*How do you fit a curve? What's a statistic? and most importantly **what is being squared?*** 

These are important questions that you might encounter when you look up curve fitting, but before we answer all of those, let's first begin by answering the most common and important question here: 

<style>
.question-box {
  border: 2px solid #a463bf;
  border-radius: 8px;
  padding: 20px;
  margin-bottom: 25px;
  background-color: #f9f9f9;
}

.question-box h3 {
  color: #a463bf;
  margin-top: 0;
  font-size: 1.3em;
  font-weight: 700;
}

.question-content {
  font-size: 1.05em;
  line-height: 1.6;
  color: #333;
  margin-bottom: 15px;
}

details {
  border: 1px solid #ddd;
  border-radius: 6px;
  margin-top: 15px;
  background-color: #fff;
}

details > summary {
  cursor: pointer;
  padding: 12px 15px;
  background-color: #a463bf;
  color: white;
  font-weight: 600;
  font-size: 1.05em;
  border-radius: 5px;
  transition: background-color 0.2s;
}

details > summary:hover {
  background-color: #8b4ba3;
}

details[open] > summary {
  border-radius: 5px 5px 0 0;
  margin-bottom: 15px;
}

.solution-content {
  padding: 15px;
  line-height: 1.7;
  color: #333;
}

.solution-content p {
  margin-bottom: 10px;
}

.page-header {
  text-align: center;
  margin-bottom: 40px;
  padding-bottom: 20px;
  border-bottom: 3px solid #a463bf;
}

.page-header h1 {
  color: #a463bf;
  margin-bottom: 10px;
}

.page-header p {
  color: #666;
  font-size: 1.1em;
}

/* Dark mode styles */
html[data-theme='dark'] .question-box {
  background-color: #1a1a1a;
  border-color: #2698ba;
}

html[data-theme='dark'] .question-box h3 {
  color: #2698ba;
}

html[data-theme='dark'] .question-content {
  color: #e0e0e0;
}

html[data-theme='dark'] details {
  background-color: #2a2a2a;
  border-color: #444;
}

html[data-theme='dark'] details > summary {
  background-color: #2698ba;
}

html[data-theme='dark'] details > summary:hover {
  background-color: #3bb5d9;
}

html[data-theme='dark'] .solution-content {
  color: #e0e0e0;
}

html[data-theme='dark'] .page-header h1 {
  color: #2698ba;
}

html[data-theme='dark'] .page-header p {
  color: #b0b0b0;
}
</style>

<div class="question-box" markdown="1">

### What is being squared?

**Nothing!** This is a trick question used by cruel people. Unfortunately, is a great example of an abuse of notation: $$\chi^2$$ is a variable, just like $$x$$ or $$y$$. We could have called it $$\chi$$, but for historical reasons, [Pearson called it $$\chi^2$$.](https://www.tandfonline.com/doi/abs/10.1080/14786440009463897)

</div>


Now that we've tackled this important question, let us motivate the statistic with an example. Suppose that you're out there trying to measure the resistance of a resistor by studying its Voltage vs Current using the relation $$V=I R$$. Unfortunately, your equipment is not up to the standard you'd expect: it produces noisy data and you observe the following


```plotly
{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 11,
      "mode": "markers",
      "name": "Measured Data",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Voltage vs Current (Noisy Measurements)"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
```
*If only* this was a nice straight line, then using $$V=I R$$, you can immediately extract the resistance by finding its slope. However, the real world is messy and unpredictable. However, what if you could extract a straight line? A "line of best fit" that passes through the points **as close as possible**. If you have that line, then the resistance is *approximately* the slope of that line. So a natural question to ask is **how can one find this line?**

Let's denote this arbitrary line our **model** $$f(x;m,c)$$. It is a function of the variable $$x$$ (which is current here) but depends on the **parameters** $$m$$ and $$c$$ through


$$ f(x;m,c)= m x +c.$$

The two parameters $$(m,c)$$ uniquely determines a line and therefore our special model is called a linear model. In principle, there are infinitely many values of $$(m,c)$$ and it is our goal to comb through this forest of $$m$$ and $$c$$ to find the correct $$m_0$$ and $$c_0$$ for which the line $$f(x;m_0,c_0)$$ passes the "closest" to our data points. On that note, *What does "closest" even mean*?

<div class="question-box" markdown="1">

### What does closest mean?

Suppose you've collected $$N$$ data points $$y_i$$ for every input (control variable) value $$x_i$$ where $$ i=1,2,\dots,N$$, then for every data point we can find $$c$$ and $$m$$ that minimizes the distance between the data point and your model that is given by $$\vert y_i - f(x_i; m,c)\vert$$. We can mathematically write our process of minimizing the distance between our model and our data as over all the possible values of $$m$$ and $$c$$ as

$$\min_{m, c} \{\vert y_i-f(x_i;m ,c)\vert\} \text{ for all } i=1,2,\dots N.$$

This is fine in theory but not in practice, since you would have to sample all the infinitely many values of $$m$$ and $$c$$ which can take forever. However, you remember from Calculus 1 that an optimization problem can be written in terms of a derivative (set to $$0$$), so you might wonder if there is a way to implement this in our problem. It turns out, you **can** rewrite this in terms of a function whose derivative set to $$0$$ gives us the optimal solution, but we need to make a few compromises: 

1. Change $$\vert X \vert$$ to $$X^2$$, since the absolute value function is notoriously non-differentiable at $$X=0$$ and 
2. Minimize the sum of the $$\text{distance}^2$$ of all the data points instead of minimizing the distance between all of them since the first one implies the other.

(1) and (2) together is equivalent to minimizing the following function $$\mathcal{L}$$ (called the *objective function*) over all possible values of $$m$$ and $$c$$ 

$$ \mathcal{L}(m,c)=\sum_{i=1}^{N} [y_i- f(x_i; m,c)]^2.$$

The above formalism is called the [least-squares method](https://en.wikipedia.org/wiki/Least_squares), since you want to minimize the (sum) squared distance of the measured data points and the model.
</div>

In our case, minimizing the least squares is the same as finding solutions $$(m,c)$$ where the derivatives vanish. What derivatives? If our model only had one parameter $$\theta$$, then it would be an ordinary derivative $$\frac{d}{d\theta}$$. However, since we've got two parameters, we need to consider the **partial** derivatives in both, i.e we solve

$$ \begin{align} 
\frac{\partial}{\partial m} \mathcal{L}(m,c)&=0\\
\frac{\partial}{\partial c} \mathcal{L}(m,c)&=0
\end{align}$$

Solving for both simultaneously, we'll be able to find the optimum solution $m_0$ and $c_0$. In our case for a linear model , we can find the unique solution exactly (try working it out!). Here's what the least squares fit looks like for our noisy data:

```plotly
{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 15,
      "mode": "markers",
      "name": "Measured Data",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    },
    {
      "function": "2*x",
      "xmin": 0,
      "xmax": 5,
      "points": 100,
      "mode": "lines",
      "name": "Least Squares Fit m=2,c=0",
      "line": {
        "color": "#000101ff",
        "width": 3,
        "dash": "solid"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Voltage vs Current with Least Squares Fit"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
```

<div class="question-box" markdown="1">

### What lies beyond the linear model?
After dabbling with the simple linear model, we may wonder what lies beyond it: What if we'd like to fit a parabola through your data points? What if an inverse square? For cases when linearization is possible, **always linearize**, i.e if your model was $$f(x; a,b)= a x^2 +b$$, then set $$X=x^2$$ and then you have a linear model $$g(X; a,b)= a X +b$$. However, in cases where such a linearization is not possible and your model is something like $$a x^2 +b x +c$$ then you continue systematically but solve the set of equations 

$$ \begin{align} 
\frac{\partial}{\partial a} \mathcal{L}(a,b,c)&=0\\
\frac{\partial}{\partial b} \mathcal{L}(a,b,c)&=0\\
\frac{\partial}{\partial c} \mathcal{L}(a,b,c)&=0
\end{align}$$

<details markdown="1">
<summary>Model with multiple parameters and multiple inputs</summary>
If you had a general model with $$M$$ parameters $$\theta_1,\theta_2,\dots,\theta_M$$ that takes in $$P$$ inputs $$\boldsymbol{x}=(x_1,x_2,\dots, x_p)$$ given by $$f(\boldsymbol{x}; \boldsymbol{\theta})$$ that we can gather as an array or vector commonly denoted with a bold $$\boldsymbol{\theta}$$

$$ \boldsymbol{\theta}=(\theta_1,\theta_2,\dots \theta_M)$$

Then we solve the set of equations 

$$ \begin{align} 
\frac{\partial}{\partial \theta_1} \mathcal{L}(\boldsymbol{\theta})&=0\\
\frac{\partial}{\partial \theta_2} \mathcal{L}(\boldsymbol{\theta})&=0\\
\vdots & =0 \\
\frac{\partial}{\partial \theta_M} \mathcal{L}(\boldsymbol{\theta})&=0\\
\end{align}$$

where 

$$\mathcal{L}(\boldsymbol{\theta})= \sum_{i=1}^{N} [y_i - f(\boldsymbol{x}_i; \boldsymbol{\theta})]^2$$

and our optimal solution $$\boldsymbol{\theta}$$ is the function that is the closest to our data points.
</details>
</div>

Now that we have laid out the ideas behind curve fitting, we can now answer the more physical question of fitting data **with uncertainties**. You might look at the figure above and ask yourself *isn't the fact that the data fails to lie on the line the uncertainty we are correcting for?* and you would be almost right. The only way to answer that through **doing it again**. If you (or your friend) does the experiment again by performing another trial (trial 2) collecting the same number of data points, then exactly one of the following two things can happen

1. The new data points lie exactly as they did earlier: they lie around but not on a line or 
2. The new data points look completely different than earlier but still lie around but not on a line.

In experiments, you find the former 

```plotly
{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 15,
      "mode": "markers",
      "name": "Trial 1",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "First Measurement"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
```

```plotly
{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 15,
      "mode": "markers",
      "name": "Trial 2",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Second Measurement"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
```

This is where you put on your thinking cap and recognize that each of the data point you measure is really one possible outcome that you experience. Like Dr. Strange using his ability to view all the possible multiverses, you view each data point you see as a specific outcome, and the collection of all possible outcomes together is what you would get if you repeat the experiment infinitely many times. In this sense, we say that the the data points are **sampled from a distribution** which controls the probability of it showing up as a special number.

<div class="question-box" markdown="1">

### What distribution does the data come from?

This question is a very hard question that is impossible to answer exactly. The simplest answer is to make an educated guess. For most experiments, if your error is "random" enough, you can assume that the data point comes from a normal distribution $$\mathcal{N}(\mu,\sigma)$$. The normal distribution or the "bell shaped curve" is given by the following equation

$$\mathcal{N}(\mu,\sigma)= \frac{1}{\sqrt{2\sigma^2 \pi}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}$$

and is the renowned "Gaussian distribution" but normalized (i.e the area under this curve is $$=1$$). This means that **associated with each data point $$y_i$$, there is a corresponding $$\sigma_i$$**. You can guesstimate this $$\sigma_i$$ or you can be more systematic and compute it using the standard error of the mean. How you ask? 

Think of your data put into an excel sheet; If you imagine an excel sheet keeping track of your measured data with each row being the trial number and each column being the measured value for the $$i^{\rm}$$ data point $$y_i$$, then isolating a column (say the first column of $$y_1$$) over **all measurement numbers** gives you the different possible values $$y_i$$ takes. Associated with each measured data point $$y_i$$, there is a standard error of the mean $$\sigma_i$$ that you can compute by finding 

 $$\sigma_i=\text{standard deviation} / \sqrt{\text{number of trials}}$$ 
 
of the corresponding column (using [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction)). 

The width or the error of each measured data point $$y_i$$ is what we denote as $$\sigma_i$$ and is what we graphically represent as error bars. The idea is to collapse the information of multiple plots by aggregating (finding the average) of each $$y_i$$ value over all of its trials (think finding the average of a specific column) into one single plot as 

```plotly
{
  "data": [
    {
      "function": "randn(2*x, 1)",
      "xmin": 0,
      "xmax": 5,
      "points": 15,
      "mode": "markers",
      "name": "Data ",
      "marker": {
        "size": 10,
        "color": "#a463bf"
      },
      "error_y": {
        "type": "constant",
        "value": 1.0,
        "visible": true,
        "color": "#a463bf",
        "thickness": 2,
        "width": 4
      }
    }
  ],
  "layout": {
    "title": {
      "text": "Voltage vs Current with Error Bars"
    },
    "xaxis": {
      "title": "Current (A)"
    },
    "yaxis": {
      "title": "Voltage (V)"
    }
  }
}
```
where we **represent** the standard error of the mean for each value $$y_i$$ with its error bars.
</div>
Now, the question of fitting a line through our data has an added layer of *nuance* to it: **How do we factor in the *information* of the error bars into our fit?** 